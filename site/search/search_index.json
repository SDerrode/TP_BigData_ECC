{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"TP Big Data , ECC \u00b6 Cet espace recueille les slides de cours et les fichiers n\u00e9cessaires \u00e0 la r\u00e9alisation des travaux pratiques du cours de Big Data , enseign\u00e9s \u00e0 l\u2019\u00c9cole Centrale de Casablanca. St\u00e9phane Derrode , Prof. Ecole Centrale de Lyon. Informations g\u00e9n\u00e9rales \u00b6 Volume de cours et travail personnel : 9 s\u00e9ances de 2h de cours (cf d\u00e9tails ci-dessous); \u00e0 cela s\u2019ajoutent des homeworks \u00e0 faire entre 2 s\u00e9ances (pour un \u00e9quivalent d\u2019environ 2h de travail hebdomadaire). \u00c9valuation : une fiche de synth\u00e8se (FS) \u00e0 r\u00e9aliser. Voir les explications donn\u00e9es \u00e0 la fin du premier cours, et le patron Markdown pour r\u00e9diger la FS sur le r\u00e9pertoire Template_FS ; des exercices de TP et des travaux personnels \u00e0 rendre r\u00e9guli\u00e8rement (le travail \u00e0 faire est sp\u00e9cifi\u00e9 \u00e0 chaque s\u00e9ance); un examen \u00e9crit de 2h en fin de s\u00e9quence (la date reste \u00e0 pr\u00e9ciser). D\u00e9roul\u00e9 des s\u00e9quences \u00b6 S\u00e9ance #1 - Introduction au Big Data \u00b6 Objectif du cours Introduction au Big Data : enjeux \u00e9thiques, \u00e9conomiques, scientifiques. Pr\u00e9sentation de la FS par groupe de 4 \u00e0 5 \u00e9tudiants. Celle-ci doit \u00eatre pr\u00e9par\u00e9e tout au long du module, la date de remise est fix\u00e9e \u00e0 la date d\u2019examen. Les consignes pour la r\u00e9daction (utilisation de Markdown et de git ) et le rendu sont d\u00e9taill\u00e9s dans le r\u00e9pertoire Template_FS . Travail hebdomadaire \u00e0 faire pour la semaine suivante Formez les groupes, d\u00e9posez les noms et les adresses mail sur un fichier h\u00e9berg\u00e9 sur cette plateforme de partage de fichiers . Commencez \u00e0 r\u00e9fl\u00e9chir au sujet de votre FS. Regardez les vid\u00e9os non vues en cours, dont les liens figurent dans les slides, et certaines des vid\u00e9os suivantes (cela pourra vous donner des id\u00e9es sur le sujet de votre FS) : Le Big Data pour mieux nous comprendre Let\u2019s pool our medical data Why privacy matters? Big Data will impact every part of your life Big data and dangerous ideas Big Data and the Rise of Augmented Intelligence How Big Data Can Influence Decisions That Actually Matter Is Big Data Killing Creativity? Analyzing and modeling complex and big data How to Monetize Big Data How to predict the future with big data S\u00e9ance #2 - Git \u00b6 Objectif du cours Apprentissage de git , avec l\u2019outil GitHub Desktop et directement en ligne de commandes (avec un Terminal ). Les sc\u00e9narios des tutoriels sont disponibles dans le dossier tuto-git-gitlab . Travail hebdomadaire \u00e0 faire pour la semaine suivante Apr\u00e8s avoir cr\u00e9\u00e9 un compte sur la plateforme GitLab , cr\u00e9ez un projet priv\u00e9 pour la FS (me d\u00e9signer reporter de chaque FS, mon pseudo : SDerrode ), selon les modalit\u00e9s expos\u00e9es dans le dossier Template_FS . Cela me permettra de voir vos travaux (espionnage de votre travail !). Pr\u00e9paration de la s\u00e9ance #3 : L\u2019open data \u00e0 la loupe . Linked Open Data - What is it? How we found the worst place to park in New York City . Demand on a more open-source government L\u2019Open Data, Avenir des Big Data S\u00e9ance #3 - Open Data et Linked Data \u00b6 Objectif du cours Introduction \u00e0 l\u2019 Open Data et aux donn\u00e9es li\u00e9es ( linked data ). Pr\u00e9sentation du TP1 sur SparQL . Vid\u00e9os vues en cours : Tim Berners-Lee : The next Web What is Linked Data? Travail hebdomadaire \u00e0 faire pour la semaine suivante Pr\u00e9parer les requ\u00eates 2 \u00e0 6 du TP, la solution de la requ\u00eate 1 est d\u00e9j\u00e0 donn\u00e9e dans l\u2019\u00e9nonc\u00e9. N\u2019h\u00e9sitez pas \u00e0 sauvegarder vos requ\u00eates dans un fichier texte (au format markdown ?) pour pouvoir les rejouer plus tard. S\u00e9ance #4 - TP Linked Data et SparQL \u00b6 Travaux pratiques Terminer le TP #1 sur SparQL (requ\u00eates 7 \u00e0 14). Commencez le travail hebdomadaire pour la semaine suivante (travail \u00e0 rendre). Travail hebdomadaire \u00e0 faire pour la semaine suivante Choisissez une base de donn\u00e9es linked data (autre que DBPedia ) et inventez deux requ\u00eates sur cette base. Vous pouvez utiliser une des bases vues en cours, ou en chercher un nouvelle \u00e0 partir de ces sites : The Linked Open Data Cloud Linked open vocabularies R\u00e9digez un petit rapport (rapport individuel de 2 pages maxi au format Markdown ) comprenant, pour chaque requ\u00eate : Le dessin RDF de la requ\u00eate (vous pouvez utiliser draw.io pour dessiner les graphiques), Le code SparQL de la requ\u00eate (que je puisse la rejouer dans Yasgui ), Les 3 ou 4 premiers r\u00e9sultats de la requ\u00eate (copie d\u2019\u00e9cran). N\u2019oubliez pas de d\u00e9poser votre nom sur le rapport, et de m\u2019envoyer le rapport (fichier Markdown et figures) sous forme d\u2019un SEUL fichier compress\u00e9 (nomm\u00e9 nom_rendu1.zip ), par mail \u00e0 St\u00e9phane Derrode avant le d\u00e9but du cours suivant . Votre mail doit avoir pour objet [ECC - Rendu 1] . Ce rapport sera \u00e9valu\u00e9 et comptera dans votre note finale. La note prendra en compte l\u2019originalit\u00e9 de la requ\u00eate, et la qualit\u00e9 de son \u00e9criture bien s\u00fbr ! S\u00e9ance #5 - Hadoop \u00b6 Objectif du cours Pr\u00e9sentation du framework Hadoop / map-reduce Pr\u00e9sentation du TP #2 qui se d\u00e9roulera en s\u00e9ance #6. Travail hebdomadaire \u00e0 faire pour la semaine suivante R\u00e9alisez la partie 1 du TP #2 . En pr\u00e9paration \u00e0 la partie 2 de ce TP, installez Docker et le container Docker Linux/Hadoop (le suite de cette partie sera faite en s\u00e9ance #6) selon les consignes. Attention, le t\u00e9l\u00e9chargement est tr\u00e8s volumineux et n\u00e9cessite une machine avec au moins 2 GO libre sur votre disque dur. S\u00e9ance #6 - TP Hadoop \u00b6 Travaux pratiques Avancer sur le TP #2 sur Hadoop map-reduce . Pensez \u00e0 prendre des notes de votre travail, car vous devrez r\u00e9diger un CR ( cf ci-dessous). Travail hebdomadaire \u00e0 faire pour la semaine suivante Terminer les questions du TP #2 et faire un compte-rendu personnel de TP. N\u2019oubliez pas de d\u00e9poser votre nom sur le rapport, et de m\u2019envoyer le rapport (fichier Markdown et figures) sous forme d\u2019un SEUL fichier compress\u00e9 (nomm\u00e9 nom_rendu2.zip ), par mail \u00e0 St\u00e9phane Derrode avant le d\u00e9but du cours suivant . Votre mail doit avoir pour objet [ECC - Rendu 2] . Ce rapport sera \u00e9valu\u00e9 et comptera dans votre note finale. La note prendra en compte la qualit\u00e9 et la clart\u00e9 du code (qui doit \u00eatre l\u00e9g\u00e8rement comment\u00e9, avec des noms de variables qui donnent du sens \u00e0 votre programme) ! S\u00e9ance #7 - Spark \u00b6 Objectif du cours Pr\u00e9sentation du framework Spark . Pr\u00e9sentation du TP #3 qui se d\u00e9roulera en s\u00e9ance #8. Travail hebdomadaire \u00e0 faire pour la semaine suivante R\u00e9alisez la partie 1 du TP #3 , concernant la programmation fonctionnelle en Python . Commencez la partie 2 du TP #3 , jusqu\u2019\u00e0 la la section wordcount en Spark (y compris cette section). S\u00e9ance #8 - TP Spark \u00b6 Travaux pratiques Terminer le TP #3 , depuis la section Tester les scripts du cours et faire un compte-rendu personnel de TP. La partie 4 de ce TP (portant sur Spark streaming ) est optionnelle. Travail hebdomadaire \u00e0 faire pour la semaine suivante Pour le CR : n\u2019oubliez pas de d\u00e9poser votre nom sur le rapport, et de m\u2019envoyer le rapport (fichier Markdown et figures) sous forme d\u2019un SEUL fichier compress\u00e9 (nomm\u00e9 nom_rendu3.zip ), par mail \u00e0 St\u00e9phane Derrode avant le d\u00e9but du cours suivant . Votre mail doit avoir pour objet [ECC - Rendu 3] . Ce rapport sera \u00e9valu\u00e9 et comptera dans votre note finale. La note prendra en compte la qualit\u00e9 et la clart\u00e9 du code (qui doit \u00eatre l\u00e9g\u00e8rement comment\u00e9, avec des noms de variables qui donnent du sens \u00e0 votre programme) ! S\u00e9ance #9 - Examen \u00b6 Les conditions d\u2019examens seront pr\u00e9cis\u00e9es plus tard.","title":"Home"},{"location":"#tp-big-data-ecc","text":"Cet espace recueille les slides de cours et les fichiers n\u00e9cessaires \u00e0 la r\u00e9alisation des travaux pratiques du cours de Big Data , enseign\u00e9s \u00e0 l\u2019\u00c9cole Centrale de Casablanca. St\u00e9phane Derrode , Prof. Ecole Centrale de Lyon.","title":"TP Big Data, ECC"},{"location":"#informations-generales","text":"Volume de cours et travail personnel : 9 s\u00e9ances de 2h de cours (cf d\u00e9tails ci-dessous); \u00e0 cela s\u2019ajoutent des homeworks \u00e0 faire entre 2 s\u00e9ances (pour un \u00e9quivalent d\u2019environ 2h de travail hebdomadaire). \u00c9valuation : une fiche de synth\u00e8se (FS) \u00e0 r\u00e9aliser. Voir les explications donn\u00e9es \u00e0 la fin du premier cours, et le patron Markdown pour r\u00e9diger la FS sur le r\u00e9pertoire Template_FS ; des exercices de TP et des travaux personnels \u00e0 rendre r\u00e9guli\u00e8rement (le travail \u00e0 faire est sp\u00e9cifi\u00e9 \u00e0 chaque s\u00e9ance); un examen \u00e9crit de 2h en fin de s\u00e9quence (la date reste \u00e0 pr\u00e9ciser).","title":"Informations g\u00e9n\u00e9rales"},{"location":"#deroule-des-sequences","text":"","title":"D\u00e9roul\u00e9 des s\u00e9quences"},{"location":"#seance-1-introduction-au-big-data","text":"Objectif du cours Introduction au Big Data : enjeux \u00e9thiques, \u00e9conomiques, scientifiques. Pr\u00e9sentation de la FS par groupe de 4 \u00e0 5 \u00e9tudiants. Celle-ci doit \u00eatre pr\u00e9par\u00e9e tout au long du module, la date de remise est fix\u00e9e \u00e0 la date d\u2019examen. Les consignes pour la r\u00e9daction (utilisation de Markdown et de git ) et le rendu sont d\u00e9taill\u00e9s dans le r\u00e9pertoire Template_FS . Travail hebdomadaire \u00e0 faire pour la semaine suivante Formez les groupes, d\u00e9posez les noms et les adresses mail sur un fichier h\u00e9berg\u00e9 sur cette plateforme de partage de fichiers . Commencez \u00e0 r\u00e9fl\u00e9chir au sujet de votre FS. Regardez les vid\u00e9os non vues en cours, dont les liens figurent dans les slides, et certaines des vid\u00e9os suivantes (cela pourra vous donner des id\u00e9es sur le sujet de votre FS) : Le Big Data pour mieux nous comprendre Let\u2019s pool our medical data Why privacy matters? Big Data will impact every part of your life Big data and dangerous ideas Big Data and the Rise of Augmented Intelligence How Big Data Can Influence Decisions That Actually Matter Is Big Data Killing Creativity? Analyzing and modeling complex and big data How to Monetize Big Data How to predict the future with big data","title":"S\u00e9ance #1 - Introduction au Big Data"},{"location":"#seance-2-git","text":"Objectif du cours Apprentissage de git , avec l\u2019outil GitHub Desktop et directement en ligne de commandes (avec un Terminal ). Les sc\u00e9narios des tutoriels sont disponibles dans le dossier tuto-git-gitlab . Travail hebdomadaire \u00e0 faire pour la semaine suivante Apr\u00e8s avoir cr\u00e9\u00e9 un compte sur la plateforme GitLab , cr\u00e9ez un projet priv\u00e9 pour la FS (me d\u00e9signer reporter de chaque FS, mon pseudo : SDerrode ), selon les modalit\u00e9s expos\u00e9es dans le dossier Template_FS . Cela me permettra de voir vos travaux (espionnage de votre travail !). Pr\u00e9paration de la s\u00e9ance #3 : L\u2019open data \u00e0 la loupe . Linked Open Data - What is it? How we found the worst place to park in New York City . Demand on a more open-source government L\u2019Open Data, Avenir des Big Data","title":"S\u00e9ance #2 - Git"},{"location":"#seance-3-open-data-et-linked-data","text":"Objectif du cours Introduction \u00e0 l\u2019 Open Data et aux donn\u00e9es li\u00e9es ( linked data ). Pr\u00e9sentation du TP1 sur SparQL . Vid\u00e9os vues en cours : Tim Berners-Lee : The next Web What is Linked Data? Travail hebdomadaire \u00e0 faire pour la semaine suivante Pr\u00e9parer les requ\u00eates 2 \u00e0 6 du TP, la solution de la requ\u00eate 1 est d\u00e9j\u00e0 donn\u00e9e dans l\u2019\u00e9nonc\u00e9. N\u2019h\u00e9sitez pas \u00e0 sauvegarder vos requ\u00eates dans un fichier texte (au format markdown ?) pour pouvoir les rejouer plus tard.","title":"S\u00e9ance #3 - Open Data et Linked Data"},{"location":"#seance-4-tp-linked-data-et-sparql","text":"Travaux pratiques Terminer le TP #1 sur SparQL (requ\u00eates 7 \u00e0 14). Commencez le travail hebdomadaire pour la semaine suivante (travail \u00e0 rendre). Travail hebdomadaire \u00e0 faire pour la semaine suivante Choisissez une base de donn\u00e9es linked data (autre que DBPedia ) et inventez deux requ\u00eates sur cette base. Vous pouvez utiliser une des bases vues en cours, ou en chercher un nouvelle \u00e0 partir de ces sites : The Linked Open Data Cloud Linked open vocabularies R\u00e9digez un petit rapport (rapport individuel de 2 pages maxi au format Markdown ) comprenant, pour chaque requ\u00eate : Le dessin RDF de la requ\u00eate (vous pouvez utiliser draw.io pour dessiner les graphiques), Le code SparQL de la requ\u00eate (que je puisse la rejouer dans Yasgui ), Les 3 ou 4 premiers r\u00e9sultats de la requ\u00eate (copie d\u2019\u00e9cran). N\u2019oubliez pas de d\u00e9poser votre nom sur le rapport, et de m\u2019envoyer le rapport (fichier Markdown et figures) sous forme d\u2019un SEUL fichier compress\u00e9 (nomm\u00e9 nom_rendu1.zip ), par mail \u00e0 St\u00e9phane Derrode avant le d\u00e9but du cours suivant . Votre mail doit avoir pour objet [ECC - Rendu 1] . Ce rapport sera \u00e9valu\u00e9 et comptera dans votre note finale. La note prendra en compte l\u2019originalit\u00e9 de la requ\u00eate, et la qualit\u00e9 de son \u00e9criture bien s\u00fbr !","title":"S\u00e9ance #4 - TP Linked Data et SparQL"},{"location":"#seance-5-hadoop","text":"Objectif du cours Pr\u00e9sentation du framework Hadoop / map-reduce Pr\u00e9sentation du TP #2 qui se d\u00e9roulera en s\u00e9ance #6. Travail hebdomadaire \u00e0 faire pour la semaine suivante R\u00e9alisez la partie 1 du TP #2 . En pr\u00e9paration \u00e0 la partie 2 de ce TP, installez Docker et le container Docker Linux/Hadoop (le suite de cette partie sera faite en s\u00e9ance #6) selon les consignes. Attention, le t\u00e9l\u00e9chargement est tr\u00e8s volumineux et n\u00e9cessite une machine avec au moins 2 GO libre sur votre disque dur.","title":"S\u00e9ance #5 - Hadoop"},{"location":"#seance-6-tp-hadoop","text":"Travaux pratiques Avancer sur le TP #2 sur Hadoop map-reduce . Pensez \u00e0 prendre des notes de votre travail, car vous devrez r\u00e9diger un CR ( cf ci-dessous). Travail hebdomadaire \u00e0 faire pour la semaine suivante Terminer les questions du TP #2 et faire un compte-rendu personnel de TP. N\u2019oubliez pas de d\u00e9poser votre nom sur le rapport, et de m\u2019envoyer le rapport (fichier Markdown et figures) sous forme d\u2019un SEUL fichier compress\u00e9 (nomm\u00e9 nom_rendu2.zip ), par mail \u00e0 St\u00e9phane Derrode avant le d\u00e9but du cours suivant . Votre mail doit avoir pour objet [ECC - Rendu 2] . Ce rapport sera \u00e9valu\u00e9 et comptera dans votre note finale. La note prendra en compte la qualit\u00e9 et la clart\u00e9 du code (qui doit \u00eatre l\u00e9g\u00e8rement comment\u00e9, avec des noms de variables qui donnent du sens \u00e0 votre programme) !","title":"S\u00e9ance #6 - TP Hadoop"},{"location":"#seance-7-spark","text":"Objectif du cours Pr\u00e9sentation du framework Spark . Pr\u00e9sentation du TP #3 qui se d\u00e9roulera en s\u00e9ance #8. Travail hebdomadaire \u00e0 faire pour la semaine suivante R\u00e9alisez la partie 1 du TP #3 , concernant la programmation fonctionnelle en Python . Commencez la partie 2 du TP #3 , jusqu\u2019\u00e0 la la section wordcount en Spark (y compris cette section).","title":"S\u00e9ance #7 - Spark"},{"location":"#seance-8-tp-spark","text":"Travaux pratiques Terminer le TP #3 , depuis la section Tester les scripts du cours et faire un compte-rendu personnel de TP. La partie 4 de ce TP (portant sur Spark streaming ) est optionnelle. Travail hebdomadaire \u00e0 faire pour la semaine suivante Pour le CR : n\u2019oubliez pas de d\u00e9poser votre nom sur le rapport, et de m\u2019envoyer le rapport (fichier Markdown et figures) sous forme d\u2019un SEUL fichier compress\u00e9 (nomm\u00e9 nom_rendu3.zip ), par mail \u00e0 St\u00e9phane Derrode avant le d\u00e9but du cours suivant . Votre mail doit avoir pour objet [ECC - Rendu 3] . Ce rapport sera \u00e9valu\u00e9 et comptera dans votre note finale. La note prendra en compte la qualit\u00e9 et la clart\u00e9 du code (qui doit \u00eatre l\u00e9g\u00e8rement comment\u00e9, avec des noms de variables qui donnent du sens \u00e0 votre programme) !","title":"S\u00e9ance #8 - TP Spark"},{"location":"#seance-9-examen","text":"Les conditions d\u2019examens seront pr\u00e9cis\u00e9es plus tard.","title":"S\u00e9ance #9 - Examen"},{"location":"about/","text":"St\u00e9phane Derrode adresse : 2nd stair - E6 building. 36 av. Guy de Collongue - 69134 Ecully Cedex - France m\u00e9l : stephane.derrode[at]ec-lyon.fr t\u00e9l : 04 72 18 64 45, fax: 04 72 18 64 43 references : Google Scholar Research Gate HAL LIRIS ORCID DBLP","title":"About"},{"location":"TP1/","text":"TP #1 - Linked Data \u00b6 L\u2019\u00e9nonc\u00e9 de ce TP est disponible \u00e0 cette adresse : TP Linked data Le TP consiste \u00e0 r\u00e9pondre aux 14 requ\u00eates propos\u00e9es dans le client Yasgui (selon la d\u00e9monstration faite en fin de cours). Pour bien d\u00e9buter, voici la r\u00e9ponse \u00e0 la premi\u00e8re requ\u00eate, \u00e0 savoir Afficher les URLs des lyonnais (i.e. personnes n\u00e9es \u00e0 Lyon) : PREFIX dbo: <http://dbpedia.org/ontology/> PREFIX dbr: <http://dbpedia.org/resource/> SELECT ?p { ?p a dbo:Person; dbo:birthPlace dbr:Lyon. } Pour pr\u00e9parer ce TP, il vous est demand\u00e9 de r\u00e9pondre aux requ\u00eates #2 et #3 la semaine qui pr\u00e9c\u00e8de le d\u00e9but de la s\u00e9ance.","title":"TP1"},{"location":"TP1/#tp-1-linked-data","text":"L\u2019\u00e9nonc\u00e9 de ce TP est disponible \u00e0 cette adresse : TP Linked data Le TP consiste \u00e0 r\u00e9pondre aux 14 requ\u00eates propos\u00e9es dans le client Yasgui (selon la d\u00e9monstration faite en fin de cours). Pour bien d\u00e9buter, voici la r\u00e9ponse \u00e0 la premi\u00e8re requ\u00eate, \u00e0 savoir Afficher les URLs des lyonnais (i.e. personnes n\u00e9es \u00e0 Lyon) : PREFIX dbo: <http://dbpedia.org/ontology/> PREFIX dbr: <http://dbpedia.org/resource/> SELECT ?p { ?p a dbo:Person; dbo:birthPlace dbr:Lyon. } Pour pr\u00e9parer ce TP, il vous est demand\u00e9 de r\u00e9pondre aux requ\u00eates #2 et #3 la semaine qui pr\u00e9c\u00e8de le d\u00e9but de la s\u00e9ance.","title":"TP #1 - Linked Data"},{"location":"TP2/","text":"TP #2 - Hadoop map-reduce \u00b6 Ce TP fait suite au cours sur le framework libre et open source appel\u00e9 Hadoop , d\u00e9velopp\u00e9 et maintenu par la Fondation Apache . Partie 1 - wordcount map-reduce en local \u00b6 Suivez les consignes permettant d\u2019ex\u00e9cuter l\u2019algorithme map-reduce de comptage de mots en local, sur le fichier contenant un livre ( Dracula ) au format texte : Wordcount_Local.md Partie 2 - map-reduce avec Hadoop \u00b6 Pour installer Hadoop sur votre machine (\u00e0 l\u2019aide de Docker ), suivez les consignes du fichier : Install_Docker_Hadoop.md Suivez ensuite les consignes permettant de lancer le comptage de mots en tant que job map-reduce : Wordcount_Hadoop.md Enfin, r\u00e9pondez aux exercices de cet \u00e9nonc\u00e9 : Enonce_TP_Hadoop.md","title":"TP2"},{"location":"TP2/#tp-2-hadoop-map-reduce","text":"Ce TP fait suite au cours sur le framework libre et open source appel\u00e9 Hadoop , d\u00e9velopp\u00e9 et maintenu par la Fondation Apache .","title":"TP #2 - Hadoop map-reduce"},{"location":"TP2/#partie-1-wordcount-map-reduce-en-local","text":"Suivez les consignes permettant d\u2019ex\u00e9cuter l\u2019algorithme map-reduce de comptage de mots en local, sur le fichier contenant un livre ( Dracula ) au format texte : Wordcount_Local.md","title":"Partie 1 - wordcount map-reduce en local"},{"location":"TP2/#partie-2-map-reduce-avec-hadoop","text":"Pour installer Hadoop sur votre machine (\u00e0 l\u2019aide de Docker ), suivez les consignes du fichier : Install_Docker_Hadoop.md Suivez ensuite les consignes permettant de lancer le comptage de mots en tant que job map-reduce : Wordcount_Hadoop.md Enfin, r\u00e9pondez aux exercices de cet \u00e9nonc\u00e9 : Enonce_TP_Hadoop.md","title":"Partie 2 - map-reduce avec Hadoop"},{"location":"TP2/Enonce_TP_Hadoop/","text":"Sommaire Travaux pratiques sous Hadoop Pr\u00e9paration et conseils Exercice 2 - Questionner le fichier de ventes Travaux pratiques sous Hadoop \u00b6 Objectif du travail : recueillir des informations et calculer des statistiques sur des r\u00e9sultats de ventes stock\u00e9s dans le fichier purchases.txt . Pr\u00e9paration et conseils \u00b6 Dans le shell du Namenode , d\u00e9placez-vous dans le dossier ventes (\u00e0 partir du premier Terminal ) : cd ../ventes Les .. permettent de remonter dans l\u2019arborescence des dossiers d\u2019un niveau (nous \u00e9tions dans le r\u00e9pertoire wordcount ). Pour observer le contenu du fichier pr\u00e9sent dans ce dossier : more purchases.txt La commande more affiche le contenu du fichier page par page. La barre d\u2019espace permet de voir la page suivante. Pour stopper la visualisation du contenu du fichier, tapez q . Ainsi, vous pouvez constater que le fichier est organis\u00e9 en 6 colonnes : date (format YYYY-MM-DD ); heure (format hh:mm ); ville d\u2019achat; cat\u00e9gorie de l\u2019achat (parmi Book , Men\u2019s Clothing , DVDs \u2026); somme d\u00e9pens\u00e9e par le client; moyen de paiement (parmi Amex , Cash , MasterCard \u2026). Les colonnes sont s\u00e9par\u00e9es par une tabulation. Ce caract\u00e8re est cod\u00e9 par \\t en Python . Exemple : print(\"avant\\tapres\") permet d\u2019obtenir l\u2019impression de la cha\u00eene \u201c avant apres \u201d. La commande Linux wc -l purchases.txt permet d\u2019obtenir le nombre de lignes du fichier (soit 4 138 476 lignes!). Ce fichier est sans doute trop volumineux pour r\u00e9gler vos algorithmes (le temps de debug en serait largement augment\u00e9). Aussi, il est conseill\u00e9 de travailler avec un extrait du fichier : cat purchases.txt | head -n 100 > purchases_extrait100.txt Cette commande extrait les 100 premi\u00e8res lignes du fichier et les stocke dans un fichier appel\u00e9 purchases_extrait100.txt . N\u2019oubliez cependant pas de v\u00e9rifier vos scripts d\u00e9finitifs avec le fichier original. Pensez \u00e0 envoyer ces deux fichiers sur HDFS, dans le dossier input : hadoop fs -put purchases.txt input hadoop fs -put purchases_extrait100.txt input Remarque : Il n\u2019est pas possible de programmer directement dans le shell du Namenode , car celui-ci ne dispose pas d\u2019\u00e9diteur de texte. La solution consiste \u00e0 programmez vos scripts map et reduce avec votre IDE pr\u00e9f\u00e9r\u00e9e (et pourquoi pas Spyder ); stockez vos fichiers (appel\u00e9s vente_map.py et vente_reduce.py ) dans un dossier vente que vous aurez cr\u00e9\u00e9 sur votre machine (\u00e0 c\u00f4t\u00e9 du r\u00e9pertoire wordcount de la premi\u00e8re partie du TP?); envoyez vos 2 fichiers vers le Namenode : docker cp vente_map.py hadoop-master:/root/ventes docker cp vente_reduce.py hadoop-master:/root/ventes Avant de lancer le job que vous aurez pr\u00e9vu dans les fichiers vente_map.py et vente_reduce.py hadoop jar $STREAMINGJAR -input input/purchases_extrait100.txt -output sortie -mapper vente_map.py -reducer vente_reduce.py -file vente_map.py -file vente_reduce.py dans le shell du Namenode , pensez \u00e0 rendre vos scripts ex\u00e9cutables : chmod +x vente_map.py chmod +x vente_reduce.py Et si vous utilisez Windows , pensez \u00e9galement \u00e0 convertir les fins de ligne de ces 2 fichiers avec dos2unix . Vous \u00eates maintenant \u00e9quip\u00e9s pour d\u00e9velopper les scripts map-reduce permettant de r\u00e9pondre aux questions suivantes. Exercice 2 - Questionner le fichier de ventes \u00b6 Voici une liste de questions que vous pouvez aborder dans l\u2019ordre (ou non!) : Quel est le nombre d\u2019achats effectu\u00e9s pour chaque cat\u00e9gorie d\u2019achat ? Quelle est la somme totale d\u00e9pens\u00e9e pour chaque cat\u00e9gorie d\u2019achat ? Quelle somme est d\u00e9pens\u00e9e dans la ville de San Francisco dans chaque moyen de paiement ? Dans quelle ville la cat\u00e9gorie Women\u2019s Clothing a permis de g\u00e9n\u00e9rer le plus d\u2019argent Cash ? \u00c0 quelle heure les clients d\u00e9pensent-ils le plus ? Il est conseill\u00e9 de d\u00e9velopper un couple de fichiers diff\u00e9rent pour chaque question (pour garder trace de vos algorithmes).","title":"Enonce TP Hadoop"},{"location":"TP2/Enonce_TP_Hadoop/#travaux-pratiques-sous-hadoop","text":"Objectif du travail : recueillir des informations et calculer des statistiques sur des r\u00e9sultats de ventes stock\u00e9s dans le fichier purchases.txt .","title":"Travaux pratiques sous Hadoop"},{"location":"TP2/Enonce_TP_Hadoop/#preparation-et-conseils","text":"Dans le shell du Namenode , d\u00e9placez-vous dans le dossier ventes (\u00e0 partir du premier Terminal ) : cd ../ventes Les .. permettent de remonter dans l\u2019arborescence des dossiers d\u2019un niveau (nous \u00e9tions dans le r\u00e9pertoire wordcount ). Pour observer le contenu du fichier pr\u00e9sent dans ce dossier : more purchases.txt La commande more affiche le contenu du fichier page par page. La barre d\u2019espace permet de voir la page suivante. Pour stopper la visualisation du contenu du fichier, tapez q . Ainsi, vous pouvez constater que le fichier est organis\u00e9 en 6 colonnes : date (format YYYY-MM-DD ); heure (format hh:mm ); ville d\u2019achat; cat\u00e9gorie de l\u2019achat (parmi Book , Men\u2019s Clothing , DVDs \u2026); somme d\u00e9pens\u00e9e par le client; moyen de paiement (parmi Amex , Cash , MasterCard \u2026). Les colonnes sont s\u00e9par\u00e9es par une tabulation. Ce caract\u00e8re est cod\u00e9 par \\t en Python . Exemple : print(\"avant\\tapres\") permet d\u2019obtenir l\u2019impression de la cha\u00eene \u201c avant apres \u201d. La commande Linux wc -l purchases.txt permet d\u2019obtenir le nombre de lignes du fichier (soit 4 138 476 lignes!). Ce fichier est sans doute trop volumineux pour r\u00e9gler vos algorithmes (le temps de debug en serait largement augment\u00e9). Aussi, il est conseill\u00e9 de travailler avec un extrait du fichier : cat purchases.txt | head -n 100 > purchases_extrait100.txt Cette commande extrait les 100 premi\u00e8res lignes du fichier et les stocke dans un fichier appel\u00e9 purchases_extrait100.txt . N\u2019oubliez cependant pas de v\u00e9rifier vos scripts d\u00e9finitifs avec le fichier original. Pensez \u00e0 envoyer ces deux fichiers sur HDFS, dans le dossier input : hadoop fs -put purchases.txt input hadoop fs -put purchases_extrait100.txt input Remarque : Il n\u2019est pas possible de programmer directement dans le shell du Namenode , car celui-ci ne dispose pas d\u2019\u00e9diteur de texte. La solution consiste \u00e0 programmez vos scripts map et reduce avec votre IDE pr\u00e9f\u00e9r\u00e9e (et pourquoi pas Spyder ); stockez vos fichiers (appel\u00e9s vente_map.py et vente_reduce.py ) dans un dossier vente que vous aurez cr\u00e9\u00e9 sur votre machine (\u00e0 c\u00f4t\u00e9 du r\u00e9pertoire wordcount de la premi\u00e8re partie du TP?); envoyez vos 2 fichiers vers le Namenode : docker cp vente_map.py hadoop-master:/root/ventes docker cp vente_reduce.py hadoop-master:/root/ventes Avant de lancer le job que vous aurez pr\u00e9vu dans les fichiers vente_map.py et vente_reduce.py hadoop jar $STREAMINGJAR -input input/purchases_extrait100.txt -output sortie -mapper vente_map.py -reducer vente_reduce.py -file vente_map.py -file vente_reduce.py dans le shell du Namenode , pensez \u00e0 rendre vos scripts ex\u00e9cutables : chmod +x vente_map.py chmod +x vente_reduce.py Et si vous utilisez Windows , pensez \u00e9galement \u00e0 convertir les fins de ligne de ces 2 fichiers avec dos2unix . Vous \u00eates maintenant \u00e9quip\u00e9s pour d\u00e9velopper les scripts map-reduce permettant de r\u00e9pondre aux questions suivantes.","title":"Pr\u00e9paration et conseils"},{"location":"TP2/Enonce_TP_Hadoop/#exercice-2-questionner-le-fichier-de-ventes","text":"Voici une liste de questions que vous pouvez aborder dans l\u2019ordre (ou non!) : Quel est le nombre d\u2019achats effectu\u00e9s pour chaque cat\u00e9gorie d\u2019achat ? Quelle est la somme totale d\u00e9pens\u00e9e pour chaque cat\u00e9gorie d\u2019achat ? Quelle somme est d\u00e9pens\u00e9e dans la ville de San Francisco dans chaque moyen de paiement ? Dans quelle ville la cat\u00e9gorie Women\u2019s Clothing a permis de g\u00e9n\u00e9rer le plus d\u2019argent Cash ? \u00c0 quelle heure les clients d\u00e9pensent-ils le plus ? Il est conseill\u00e9 de d\u00e9velopper un couple de fichiers diff\u00e9rent pour chaque question (pour garder trace de vos algorithmes).","title":"Exercice 2 - Questionner le fichier de ventes"},{"location":"TP2/Install_Docker_Hadoop/","text":"Sommaire Installation de Hadoop via Docker Installation de Docker et des n\u0153uds Pr\u00e9paration au TP Installation de Hadoop via Docker \u00b6 Les \u00e9tapes pour installer Hadoop via Docker sont largement adapt\u00e9es de la page de Lilia Sfaxi , elles-m\u00eames reposant sur le projet github de Kai LIU . Installation de Docker et des n\u0153uds \u00b6 Pour installer Docker , merci de suivre les consignes disponibles ici , en fonction de votre syst\u00e8me d\u2019exploitation (lisez les System requirements pour v\u00e9rifier que votre machine est adapt\u00e9e). Si votre machine est trop ancienne, ou avec peu d\u2019espace disque ou m\u00e9moire RAM, il y a de bonnes chances que l\u2019installation ne fonctionne pas. Si c\u2019est le cas, soit vous pouvez travailler avec votre voisin, soit vous pouvez aller directement \u00e0 la seconde partie du TP, et r\u00e9aliser les exercices en local (sans Hadoop ). Nous allons utiliser tout au long de ce TP trois contenaires repr\u00e9sentant respectivement un n\u0153ud ma\u00eetre (le Namenode ) et deux n\u0153uds esclaves (les Datanodes ). Depuis un Terminal , t\u00e9l\u00e9chargez l\u2019image docker depuis dockerhub (volume \u00e0 t\u00e9l\u00e9charger > 1.5 GB !) : docker pull stephanederrode/docker-cluster-hadoop-spark-python:1.0 Ce container contient une distribution Linux/Ubuntu , et les librairies n\u00e9cessaires pour utiliser Hadoop et Spark . Il contient \u00e9galement Python2.7 . Cr\u00e9ez les trois contenaires \u00e0 partir de l\u2019image t\u00e9l\u00e9charg\u00e9e. Pour cela: a. Cr\u00e9ez un r\u00e9seau qui permettra de relier les trois contenaires: docker network create --driver=bridge hadoop b. Cr\u00e9ez et lancez les trois contenaires (les instructions -p permettent de faire un mapping entre les ports de la machine h\u00f4te et ceux du contenaire): docker run -itd --net=hadoop -p 50070:50070 -p 8088:8088 -p 7077:7077 -p 16010:16010 -p 9999:9999 --name hadoop-master --hostname hadoop-master stephanederrode/docker-cluster-hadoop-spark-python:1.0 docker run -itd -p 8040:8042 --net=hadoop --name hadoop-slave1 --hostname hadoop-slave1 stephanederrode/docker-cluster-hadoop-spark-python:1.0 docker run -itd -p 8041:8042 --net=hadoop --name hadoop-slave2 --hostname hadoop-slave2 stephanederrode/docker-cluster-hadoop-spark-python:1.0 Remarques Sur certaines machines, la premi\u00e8re ligne de commande ne s\u2019ex\u00e9cute pas correctement. L\u2019erreur provient sans doute du port 50070 que doit d\u00e9j\u00e0 \u00eatre utilis\u00e9 par une autre application install\u00e9e sur votre machine. Vous pouvez alors supprimer ce port de la premi\u00e8re ligne de commande : docker run -itd --net=hadoop -p 8088:8088 -p 7077:7077 -p 16010:16010 --name hadoop-master --hostname hadoop-master stephanederrode/docker-cluster-hadoop-spark-python:1.0 Le port 9999 sera utilis\u00e9 dans la partie 3 de ce TP, au sujet de Spark streaming . Pr\u00e9paration au TP \u00b6 Entrez dans le contenaire hadoop-master pour commencer \u00e0 l\u2019utiliser docker exec -it hadoop-master bash Le r\u00e9sultat de cette ex\u00e9cution sera le suivant: root@hadoop-master:~# Il s\u2019agit du shell ou bash ( Linux/Ubuntu ) du n\u0153ud ma\u00eetre. La commande ls , qui liste les fichiers et dossiers du dossier en cours, doit faire \u00e9tat des fichiers suivants : hdfs start-hadoop.sh ventes Le dossier ventes contient un fichier purchases.txt qui sera utilis\u00e9 lors de la seconde partie du TP. Remarque Ces \u00e9tapes de configuration ne doivent \u00eatre r\u00e9alis\u00e9es qu\u2019une seule fois. Pour relancer le cluster (une fois qu\u2019on a fermer et relancer son ordinateur p. ex.), il suffira de lancer l\u2019application Docker Desktop , qui lance les daemon Docker . de lancer la commande suivante : docker start hadoop-master hadoop-slave1 hadoop-slave2 Vous pouvez alors entrer dans le Namenode : docker exec -it hadoop-master bash","title":"Install Docker Hadoop"},{"location":"TP2/Install_Docker_Hadoop/#installation-de-hadoop-via-docker","text":"Les \u00e9tapes pour installer Hadoop via Docker sont largement adapt\u00e9es de la page de Lilia Sfaxi , elles-m\u00eames reposant sur le projet github de Kai LIU .","title":"Installation de Hadoop via Docker"},{"location":"TP2/Install_Docker_Hadoop/#installation-de-docker-et-des-nuds","text":"Pour installer Docker , merci de suivre les consignes disponibles ici , en fonction de votre syst\u00e8me d\u2019exploitation (lisez les System requirements pour v\u00e9rifier que votre machine est adapt\u00e9e). Si votre machine est trop ancienne, ou avec peu d\u2019espace disque ou m\u00e9moire RAM, il y a de bonnes chances que l\u2019installation ne fonctionne pas. Si c\u2019est le cas, soit vous pouvez travailler avec votre voisin, soit vous pouvez aller directement \u00e0 la seconde partie du TP, et r\u00e9aliser les exercices en local (sans Hadoop ). Nous allons utiliser tout au long de ce TP trois contenaires repr\u00e9sentant respectivement un n\u0153ud ma\u00eetre (le Namenode ) et deux n\u0153uds esclaves (les Datanodes ). Depuis un Terminal , t\u00e9l\u00e9chargez l\u2019image docker depuis dockerhub (volume \u00e0 t\u00e9l\u00e9charger > 1.5 GB !) : docker pull stephanederrode/docker-cluster-hadoop-spark-python:1.0 Ce container contient une distribution Linux/Ubuntu , et les librairies n\u00e9cessaires pour utiliser Hadoop et Spark . Il contient \u00e9galement Python2.7 . Cr\u00e9ez les trois contenaires \u00e0 partir de l\u2019image t\u00e9l\u00e9charg\u00e9e. Pour cela: a. Cr\u00e9ez un r\u00e9seau qui permettra de relier les trois contenaires: docker network create --driver=bridge hadoop b. Cr\u00e9ez et lancez les trois contenaires (les instructions -p permettent de faire un mapping entre les ports de la machine h\u00f4te et ceux du contenaire): docker run -itd --net=hadoop -p 50070:50070 -p 8088:8088 -p 7077:7077 -p 16010:16010 -p 9999:9999 --name hadoop-master --hostname hadoop-master stephanederrode/docker-cluster-hadoop-spark-python:1.0 docker run -itd -p 8040:8042 --net=hadoop --name hadoop-slave1 --hostname hadoop-slave1 stephanederrode/docker-cluster-hadoop-spark-python:1.0 docker run -itd -p 8041:8042 --net=hadoop --name hadoop-slave2 --hostname hadoop-slave2 stephanederrode/docker-cluster-hadoop-spark-python:1.0 Remarques Sur certaines machines, la premi\u00e8re ligne de commande ne s\u2019ex\u00e9cute pas correctement. L\u2019erreur provient sans doute du port 50070 que doit d\u00e9j\u00e0 \u00eatre utilis\u00e9 par une autre application install\u00e9e sur votre machine. Vous pouvez alors supprimer ce port de la premi\u00e8re ligne de commande : docker run -itd --net=hadoop -p 8088:8088 -p 7077:7077 -p 16010:16010 --name hadoop-master --hostname hadoop-master stephanederrode/docker-cluster-hadoop-spark-python:1.0 Le port 9999 sera utilis\u00e9 dans la partie 3 de ce TP, au sujet de Spark streaming .","title":"Installation de Docker et des n\u0153uds"},{"location":"TP2/Install_Docker_Hadoop/#preparation-au-tp","text":"Entrez dans le contenaire hadoop-master pour commencer \u00e0 l\u2019utiliser docker exec -it hadoop-master bash Le r\u00e9sultat de cette ex\u00e9cution sera le suivant: root@hadoop-master:~# Il s\u2019agit du shell ou bash ( Linux/Ubuntu ) du n\u0153ud ma\u00eetre. La commande ls , qui liste les fichiers et dossiers du dossier en cours, doit faire \u00e9tat des fichiers suivants : hdfs start-hadoop.sh ventes Le dossier ventes contient un fichier purchases.txt qui sera utilis\u00e9 lors de la seconde partie du TP. Remarque Ces \u00e9tapes de configuration ne doivent \u00eatre r\u00e9alis\u00e9es qu\u2019une seule fois. Pour relancer le cluster (une fois qu\u2019on a fermer et relancer son ordinateur p. ex.), il suffira de lancer l\u2019application Docker Desktop , qui lance les daemon Docker . de lancer la commande suivante : docker start hadoop-master hadoop-slave1 hadoop-slave2 Vous pouvez alors entrer dans le Namenode : docker exec -it hadoop-master bash","title":"Pr\u00e9paration au TP"},{"location":"TP2/Wordcount_Hadoop/","text":"Sommaire Map-reduce, avec Hadoop Lancement du daemon Hadoop Pr\u00e9paration des fichiers pour wordcount Wordcount avec Hadoop Monitoring du cluster et des jobs Map-reduce, avec Hadoop \u00b6 \u00c9tant donn\u00e9e l\u2019installation pr\u00e9c\u00e9dente, nous allons exploiter le parall\u00e9lisme de votre processeur, souvent constitu\u00e9 de 4 c\u0153urs, et donc susceptible de lancer 4 instructions en parall\u00e8le. Parmi ces 4 c\u0153urs, nous n\u2019en exploiterons que 3 (1 pour le Namenode et 2 pour les Datanodes ), le dernier c\u0153ur \u00e9tant \u00e0 disposition de votre machine pour toutes les autres t\u00e2ches. Lancement du daemon Hadoop \u00b6 La premi\u00e8re chose \u00e0 faire sur le Terminal connect\u00e9 au hadoop-master est de lancer les daemon Hadoop : ./start-hadoop.sh Le r\u00e9sultat de l\u2019ex\u00e9cution de ce script ressemblera \u00e0 : Starting namenodes on [hadoop-master] hadoop-master: Warning: Permanently added 'hadoop-master,172.18.0.4' (ECDSA) to the list of known hosts. hadoop-master: starting namenode, logging to /usr/local/hadoop/logs/hadoop-root-namenode-hadoop-master.out hadoop-slave2: Warning: Permanently added 'hadoop-slave2,172.18.0.2' (ECDSA) to the list of known hosts. hadoop-slave1: Warning: Permanently added 'hadoop-slave1,172.18.0.3' (ECDSA) to the list of known hosts. hadoop-slave2: starting datanode, logging to /usr/local/hadoop/logs/hadoop-root-datanode-hadoop-slave2.out hadoop-slave1: starting datanode, logging to /usr/local/hadoop/logs/hadoop-root-datanode-hadoop-slave1.out Starting secondary namenodes [0.0.0.0] 0.0.0.0: starting secondarynamenode, logging to /usr/local/hadoop/logs/hadoop-root-secondarynamenode-hadoop-master.out starting yarn daemons starting resourcemanager, logging to /usr/local/hadoop/logs/yarn--resourcemanager-hadoop-master.out hadoop-slave1: Warning: Permanently added 'hadoop-slave1,172.18.0.3' (ECDSA) to the list of known hosts. hadoop-slave2: Warning: Permanently added 'hadoop-slave2,172.18.0.2' (ECDSA) to the list of known hosts. hadoop-slave1: starting nodemanager, logging to /usr/local/hadoop/logs/yarn-root-nodemanager-hadoop-slave1.out hadoop-slave2: starting nodemanager, logging to /usr/local/hadoop/logs/yarn-root-nodemanager-hadoop-slave2.out Pr\u00e9paration des fichiers pour wordcount \u00b6 Remarque importante Le Terminal pointe sur un syst\u00e8me Linux qui a son propre mode de stockage de fichier (appel\u00e9 ext3 ). Il est alors possible de cr\u00e9er des dossiers, de d\u00e9poser des fichiers, de les effacer\u2026 avec les commandes Linux traditionnelles ( mkdir , rm \u2026). Notons qu\u2019il n\u2019existe pas d\u2019\u00e9diteur de texte int\u00e9gr\u00e9 au container que nous venons d\u2019installer (pour \u00e9crire les scripts Python ), donc nous aurons recours \u00e0 une astuce d\u00e9crite ci-dessous. C\u2019est sur cet espace que nous stockerons les scripts Python map-reduce qui seront ex\u00e9cut\u00e9s par Hadoop . Par contre, les fichiers volumineux, ceux pour lesquels nous d\u00e9ploierons des algorithmes de traitement, seront stock\u00e9s sur une partie de votre disque dur g\u00e9r\u00e9e par HDFS ( Hadoop Distributed File System ). \u00c0 l\u2019aide de commandes commen\u00e7ant par \u201c hadoop fs - + commande\u201d , il est possible de cr\u00e9er des dossiers sur HDFS , de copier des fichiers depuis Linux vers HDFS , et de rapatrier des fichiers depuis HDFS vers Linux. Laissez-vous guider\u2026 Depuis le Terminal , cr\u00e9ez un dossier wordcount et d\u00e9placez-vous dedans mkdir wordcount cd wordcount T\u00e9l\u00e9chargez depuis internet le livre dracula \u00e0 l\u2019aide de la commande wget http://www.textfiles.com/etext/FICTION/dracula Versez ce fichier volumineux sur l\u2019espace HDFS (apr\u00e8s avoir cr\u00e9er un dossier pour le recevoir) hadoop fs -mkdir -p input hadoop fs -put dracula input V\u00e9rifiez que le fichier a bien \u00e9t\u00e9 d\u00e9pos\u00e9: hadoop fs -ls input ce qui donnera quelque chose comme: Found 1 items -rw-r--r-- 2 root supergroup 844505 2020-10-16 05:02 input/dracula Supprimer le fichier dracula de votre espace Linux (on n\u2019en a plus besoin!) rm dracula Il faut maintenant rapatrier, sur notre espace Linux, les scripts mapper.py et reducer.py que nous avons manipul\u00e9s durant la premi\u00e8re partie de ce TP. Pour cela, il faut ouvrir un second Terminal (laissez le premier ouvert, il va nous resservir!), et vous d\u00e9placer dans le dossier de travail qui contient les scripts mapper.py et reducer.py , modifi\u00e9s par vos soins durant la premi\u00e8re partie. La commande suivante permet de copier ces 2 fichiers vers l\u2019espace Linux, dans le dossier wordcount docker cp mapper.py hadoop-master:/root/wordcount docker cp reducer.py hadoop-master:/root/wordcount Retenez la syntaxe, car elle vous sera utile plus tard, pour rapatrier les nouveaux scripts Python que vous aurez d\u00e9velopp\u00e9s. Revenez alors vers le premier Terminal (ne fermez pas le second, il sera utile plus tard), et v\u00e9rifiez avec la commande ls que les 2 fichiers sont bien pr\u00e9sents. Il faut maintenant rendre ces 2 scripts ex\u00e9cutables: chmod +x mapper.py chmod +x reducer.py Pour les utilisateurs de Windows uniquement : il faut aussi convertir les caract\u00e8res de saut de lignes, qui sont diff\u00e9rents entre Windows et Linux . Pour chaque fichier texte ( p. ex. , fichier.py ) que vous rapatrierez depuis votre machine sur le compte Linux , il conviendra de lancer: dos2unix fichier.py Il faudra appliquer ce protocole aux fichiers mapper.py et reducer.py , \u00e0 chaque fois que vous les aurez modifi\u00e9s sous Windows . Souvenez-vous de cette manip., car il faudra aussi la mettre en place sur vos nouveaux scripts. \u00c7a y est, nous sommes pr\u00eats \u00e0 lancer notre premier script map-reduce sous Hadoop ! Wordcount avec Hadoop \u00b6 \u00c0 partir du premier Terminal , nous allons donc lancer les scripts permettant de compter le nombre de mots sur le fichier du livre dracula . Tout d\u2019abord, stockez le lien vers la librairie permettant de programmer avec Python dans une variable syst\u00e8me : export STREAMINGJAR='/usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.2.jar' Je vous rappelle que Hadoop map-reduce fonctionne avec le langage Java ; il faut donc utiliser une biblioth\u00e8que capable de transformer des instructions Python en instruction Java . C\u2019est le r\u00f4le de cette biblioth\u00e8que hadoop-streaming-2.7.2.jar (on appelle cela un wrapper ). - Ensuite, lancez le job Hadoop avec l\u2019instruction suivante (copiez tout le bloc d\u2019instrcution et collez-le dans le Terminal ): hadoop jar $STREAMINGJAR -input input/dracula -output sortie -mapper mapper.py -reducer reducer.py -file mapper.py -file reducer.py Les options -file permettent de copier les fichiers n\u00e9cessaires pour qu\u2019ils soit ex\u00e9cut\u00e9s sur tous les n\u0153uds du cluster. Le r\u00e9sultat du comptage de mots est stock\u00e9 dans le dossier sortie sous HDFS . Vous pouvez voir son contenu en lan\u00e7ant la commande: hadoop fs -ls sortie/ qui donnera quelque chose comme Found 2 items -rw-r--r-- 2 root supergroup 0 2020-10-16 06:58 sortie/_SUCCESS -rw-r--r-- 2 root supergroup 25 2020-10-16 06:58 sortie/part-00000 Le premier fichier _SUCCESS est un fichier vide (0 octet!), dont la simple pr\u00e9sence indique que le job s\u2019est termin\u00e9 avec succ\u00e8s. Le second fichier part-00000 contient le r\u00e9sultat de l\u2019algorithme. Vous pouvez visualiser les derni\u00e8res lignes du fichier avec la commande : hadoop fs -tail sortie/part-00000 ou voir tout le fichier avec la commande : hadoop fs -cat sortie/part-00000 Le r\u00e9sultat devrait \u00eatre exactement le m\u00eame que lors de la premi\u00e8re partie du TP. Remarque - N\u2019oubliez pas! : Entre 2 ex\u00e9cutions, il faut soit utiliser un nouveau nom pour le dossier sortie , soit le supprimer de la mani\u00e8re suivante : hadoop fs -rm -r -f sortie La pr\u00e9sence d\u2019un seul fichier part-0000x montre qu\u2019un seul n\u0153ud a \u00e9t\u00e9 utilis\u00e9 pour le reducer (le nombre de n\u0153uds est estim\u00e9 par le Namenode ). Il est possible de forcer le nombre de reducer : hadoop jar $STREAMINGJAR -D mapred.reduce.tasks=2 -input input/dracula -output sortie -mapper mapper.py -reducer reducer.py -file mapper.py -file reducer.py La commande : hadoop fs -ls sortie/ donnera alors : Found 3 items -rw-r--r-- 2 root supergroup 0 2020-10-17 15:24 sortie/_SUCCESS -rw-r--r-- 2 root supergroup 117444 2020-10-17 15:24 sortie/part-00000 -rw-r--r-- 2 root supergroup 118967 2020-10-17 15:24 sortie/part-00001 Monitoring du cluster et des jobs \u00b6 Hadoop offre plusieurs interfaces web pour pouvoir observer le comportement de ses diff\u00e9rentes composantes. Vous pouvez afficher ces pages en local sur votre machine gr\u00e2ce \u00e0 l\u2019option -p de la commande docker run . Le port 50070 permet d\u2019afficher les informations de votre Namenode . Le port 8088 permet d\u2019afficher les informations du resource manager (apepl\u00e9 Yarn ) et visualiser le comportement des diff\u00e9rents jobs. Une fois votre cluster lanc\u00e9 et pr\u00eat \u00e0 l\u2019emploi, utilisez votre navigateur pr\u00e9f\u00e9r\u00e9 pour observer la page http://localhost:50070 . Attention : lors de l\u2019installation, certains \u00e9tudiants auront du supprimer le mapping de ce port, ils ne leur sera donc pas possible de visualiser la page, semblable \u00e0: Prenez le temps de naviguer dans les menus et d\u2019observer les informations indiqu\u00e9es. Vous pouvez \u00e9galement visualiser l\u2019avancement et les r\u00e9sultats de vos jobs ( map-reduce ou autre) en allant \u00e0 l\u2019adresse http://localhost:8088 . Prenez le temps l\u00e0-aussi de naviguer dans les menus et d\u2019observer les informations indiqu\u00e9es. Dernier point : Il est \u00e9galement possible de voir le comportement des n\u0153uds Datanodes , en allant \u00e0 l\u2019adresse: http://localhost:8041 pour slave1 , et http://localhost:8042 pour slave2 .","title":"Wordcount Hadoop"},{"location":"TP2/Wordcount_Hadoop/#map-reduce-avec-hadoop","text":"\u00c9tant donn\u00e9e l\u2019installation pr\u00e9c\u00e9dente, nous allons exploiter le parall\u00e9lisme de votre processeur, souvent constitu\u00e9 de 4 c\u0153urs, et donc susceptible de lancer 4 instructions en parall\u00e8le. Parmi ces 4 c\u0153urs, nous n\u2019en exploiterons que 3 (1 pour le Namenode et 2 pour les Datanodes ), le dernier c\u0153ur \u00e9tant \u00e0 disposition de votre machine pour toutes les autres t\u00e2ches.","title":"Map-reduce, avec Hadoop"},{"location":"TP2/Wordcount_Hadoop/#lancement-du-daemon-hadoop","text":"La premi\u00e8re chose \u00e0 faire sur le Terminal connect\u00e9 au hadoop-master est de lancer les daemon Hadoop : ./start-hadoop.sh Le r\u00e9sultat de l\u2019ex\u00e9cution de ce script ressemblera \u00e0 : Starting namenodes on [hadoop-master] hadoop-master: Warning: Permanently added 'hadoop-master,172.18.0.4' (ECDSA) to the list of known hosts. hadoop-master: starting namenode, logging to /usr/local/hadoop/logs/hadoop-root-namenode-hadoop-master.out hadoop-slave2: Warning: Permanently added 'hadoop-slave2,172.18.0.2' (ECDSA) to the list of known hosts. hadoop-slave1: Warning: Permanently added 'hadoop-slave1,172.18.0.3' (ECDSA) to the list of known hosts. hadoop-slave2: starting datanode, logging to /usr/local/hadoop/logs/hadoop-root-datanode-hadoop-slave2.out hadoop-slave1: starting datanode, logging to /usr/local/hadoop/logs/hadoop-root-datanode-hadoop-slave1.out Starting secondary namenodes [0.0.0.0] 0.0.0.0: starting secondarynamenode, logging to /usr/local/hadoop/logs/hadoop-root-secondarynamenode-hadoop-master.out starting yarn daemons starting resourcemanager, logging to /usr/local/hadoop/logs/yarn--resourcemanager-hadoop-master.out hadoop-slave1: Warning: Permanently added 'hadoop-slave1,172.18.0.3' (ECDSA) to the list of known hosts. hadoop-slave2: Warning: Permanently added 'hadoop-slave2,172.18.0.2' (ECDSA) to the list of known hosts. hadoop-slave1: starting nodemanager, logging to /usr/local/hadoop/logs/yarn-root-nodemanager-hadoop-slave1.out hadoop-slave2: starting nodemanager, logging to /usr/local/hadoop/logs/yarn-root-nodemanager-hadoop-slave2.out","title":"Lancement du daemon Hadoop"},{"location":"TP2/Wordcount_Hadoop/#preparation-des-fichiers-pour-wordcount","text":"Remarque importante Le Terminal pointe sur un syst\u00e8me Linux qui a son propre mode de stockage de fichier (appel\u00e9 ext3 ). Il est alors possible de cr\u00e9er des dossiers, de d\u00e9poser des fichiers, de les effacer\u2026 avec les commandes Linux traditionnelles ( mkdir , rm \u2026). Notons qu\u2019il n\u2019existe pas d\u2019\u00e9diteur de texte int\u00e9gr\u00e9 au container que nous venons d\u2019installer (pour \u00e9crire les scripts Python ), donc nous aurons recours \u00e0 une astuce d\u00e9crite ci-dessous. C\u2019est sur cet espace que nous stockerons les scripts Python map-reduce qui seront ex\u00e9cut\u00e9s par Hadoop . Par contre, les fichiers volumineux, ceux pour lesquels nous d\u00e9ploierons des algorithmes de traitement, seront stock\u00e9s sur une partie de votre disque dur g\u00e9r\u00e9e par HDFS ( Hadoop Distributed File System ). \u00c0 l\u2019aide de commandes commen\u00e7ant par \u201c hadoop fs - + commande\u201d , il est possible de cr\u00e9er des dossiers sur HDFS , de copier des fichiers depuis Linux vers HDFS , et de rapatrier des fichiers depuis HDFS vers Linux. Laissez-vous guider\u2026 Depuis le Terminal , cr\u00e9ez un dossier wordcount et d\u00e9placez-vous dedans mkdir wordcount cd wordcount T\u00e9l\u00e9chargez depuis internet le livre dracula \u00e0 l\u2019aide de la commande wget http://www.textfiles.com/etext/FICTION/dracula Versez ce fichier volumineux sur l\u2019espace HDFS (apr\u00e8s avoir cr\u00e9er un dossier pour le recevoir) hadoop fs -mkdir -p input hadoop fs -put dracula input V\u00e9rifiez que le fichier a bien \u00e9t\u00e9 d\u00e9pos\u00e9: hadoop fs -ls input ce qui donnera quelque chose comme: Found 1 items -rw-r--r-- 2 root supergroup 844505 2020-10-16 05:02 input/dracula Supprimer le fichier dracula de votre espace Linux (on n\u2019en a plus besoin!) rm dracula Il faut maintenant rapatrier, sur notre espace Linux, les scripts mapper.py et reducer.py que nous avons manipul\u00e9s durant la premi\u00e8re partie de ce TP. Pour cela, il faut ouvrir un second Terminal (laissez le premier ouvert, il va nous resservir!), et vous d\u00e9placer dans le dossier de travail qui contient les scripts mapper.py et reducer.py , modifi\u00e9s par vos soins durant la premi\u00e8re partie. La commande suivante permet de copier ces 2 fichiers vers l\u2019espace Linux, dans le dossier wordcount docker cp mapper.py hadoop-master:/root/wordcount docker cp reducer.py hadoop-master:/root/wordcount Retenez la syntaxe, car elle vous sera utile plus tard, pour rapatrier les nouveaux scripts Python que vous aurez d\u00e9velopp\u00e9s. Revenez alors vers le premier Terminal (ne fermez pas le second, il sera utile plus tard), et v\u00e9rifiez avec la commande ls que les 2 fichiers sont bien pr\u00e9sents. Il faut maintenant rendre ces 2 scripts ex\u00e9cutables: chmod +x mapper.py chmod +x reducer.py Pour les utilisateurs de Windows uniquement : il faut aussi convertir les caract\u00e8res de saut de lignes, qui sont diff\u00e9rents entre Windows et Linux . Pour chaque fichier texte ( p. ex. , fichier.py ) que vous rapatrierez depuis votre machine sur le compte Linux , il conviendra de lancer: dos2unix fichier.py Il faudra appliquer ce protocole aux fichiers mapper.py et reducer.py , \u00e0 chaque fois que vous les aurez modifi\u00e9s sous Windows . Souvenez-vous de cette manip., car il faudra aussi la mettre en place sur vos nouveaux scripts. \u00c7a y est, nous sommes pr\u00eats \u00e0 lancer notre premier script map-reduce sous Hadoop !","title":"Pr\u00e9paration des fichiers pour wordcount"},{"location":"TP2/Wordcount_Hadoop/#wordcount-avec-hadoop","text":"\u00c0 partir du premier Terminal , nous allons donc lancer les scripts permettant de compter le nombre de mots sur le fichier du livre dracula . Tout d\u2019abord, stockez le lien vers la librairie permettant de programmer avec Python dans une variable syst\u00e8me : export STREAMINGJAR='/usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.2.jar' Je vous rappelle que Hadoop map-reduce fonctionne avec le langage Java ; il faut donc utiliser une biblioth\u00e8que capable de transformer des instructions Python en instruction Java . C\u2019est le r\u00f4le de cette biblioth\u00e8que hadoop-streaming-2.7.2.jar (on appelle cela un wrapper ). - Ensuite, lancez le job Hadoop avec l\u2019instruction suivante (copiez tout le bloc d\u2019instrcution et collez-le dans le Terminal ): hadoop jar $STREAMINGJAR -input input/dracula -output sortie -mapper mapper.py -reducer reducer.py -file mapper.py -file reducer.py Les options -file permettent de copier les fichiers n\u00e9cessaires pour qu\u2019ils soit ex\u00e9cut\u00e9s sur tous les n\u0153uds du cluster. Le r\u00e9sultat du comptage de mots est stock\u00e9 dans le dossier sortie sous HDFS . Vous pouvez voir son contenu en lan\u00e7ant la commande: hadoop fs -ls sortie/ qui donnera quelque chose comme Found 2 items -rw-r--r-- 2 root supergroup 0 2020-10-16 06:58 sortie/_SUCCESS -rw-r--r-- 2 root supergroup 25 2020-10-16 06:58 sortie/part-00000 Le premier fichier _SUCCESS est un fichier vide (0 octet!), dont la simple pr\u00e9sence indique que le job s\u2019est termin\u00e9 avec succ\u00e8s. Le second fichier part-00000 contient le r\u00e9sultat de l\u2019algorithme. Vous pouvez visualiser les derni\u00e8res lignes du fichier avec la commande : hadoop fs -tail sortie/part-00000 ou voir tout le fichier avec la commande : hadoop fs -cat sortie/part-00000 Le r\u00e9sultat devrait \u00eatre exactement le m\u00eame que lors de la premi\u00e8re partie du TP. Remarque - N\u2019oubliez pas! : Entre 2 ex\u00e9cutions, il faut soit utiliser un nouveau nom pour le dossier sortie , soit le supprimer de la mani\u00e8re suivante : hadoop fs -rm -r -f sortie La pr\u00e9sence d\u2019un seul fichier part-0000x montre qu\u2019un seul n\u0153ud a \u00e9t\u00e9 utilis\u00e9 pour le reducer (le nombre de n\u0153uds est estim\u00e9 par le Namenode ). Il est possible de forcer le nombre de reducer : hadoop jar $STREAMINGJAR -D mapred.reduce.tasks=2 -input input/dracula -output sortie -mapper mapper.py -reducer reducer.py -file mapper.py -file reducer.py La commande : hadoop fs -ls sortie/ donnera alors : Found 3 items -rw-r--r-- 2 root supergroup 0 2020-10-17 15:24 sortie/_SUCCESS -rw-r--r-- 2 root supergroup 117444 2020-10-17 15:24 sortie/part-00000 -rw-r--r-- 2 root supergroup 118967 2020-10-17 15:24 sortie/part-00001","title":"Wordcount avec Hadoop"},{"location":"TP2/Wordcount_Hadoop/#monitoring-du-cluster-et-des-jobs","text":"Hadoop offre plusieurs interfaces web pour pouvoir observer le comportement de ses diff\u00e9rentes composantes. Vous pouvez afficher ces pages en local sur votre machine gr\u00e2ce \u00e0 l\u2019option -p de la commande docker run . Le port 50070 permet d\u2019afficher les informations de votre Namenode . Le port 8088 permet d\u2019afficher les informations du resource manager (apepl\u00e9 Yarn ) et visualiser le comportement des diff\u00e9rents jobs. Une fois votre cluster lanc\u00e9 et pr\u00eat \u00e0 l\u2019emploi, utilisez votre navigateur pr\u00e9f\u00e9r\u00e9 pour observer la page http://localhost:50070 . Attention : lors de l\u2019installation, certains \u00e9tudiants auront du supprimer le mapping de ce port, ils ne leur sera donc pas possible de visualiser la page, semblable \u00e0: Prenez le temps de naviguer dans les menus et d\u2019observer les informations indiqu\u00e9es. Vous pouvez \u00e9galement visualiser l\u2019avancement et les r\u00e9sultats de vos jobs ( map-reduce ou autre) en allant \u00e0 l\u2019adresse http://localhost:8088 . Prenez le temps l\u00e0-aussi de naviguer dans les menus et d\u2019observer les informations indiqu\u00e9es. Dernier point : Il est \u00e9galement possible de voir le comportement des n\u0153uds Datanodes , en allant \u00e0 l\u2019adresse: http://localhost:8041 pour slave1 , et http://localhost:8042 pour slave2 .","title":"Monitoring du cluster et des jobs"},{"location":"TP2/Wordcount_Local/","text":"Sommaire Wordcount, en local R\u00e9cup\u00e9ration et lancement des scripts Python Exercice 1 - Am\u00e9lioration du wordcount Wordcount, en local \u00b6 Nous allons ici faire fonctionner l\u2019algorithme map-reduce qui compte les mots d\u2019un fichier texte, en local, i.e. sans exploiter le parall\u00e9lisme que propose le framework Hadoop . Le programme est constitu\u00e9 de deux scripts Python qui sont appel\u00e9s successivement selon la m\u00e9thode d\u00e9crite ci-dessous. Il s\u2019agit ici surtout de comprendre la logique algorithmique. R\u00e9cup\u00e9ration et lancement des scripts Python \u00b6 Pour r\u00e9cup\u00e9rer le scripts, suivez les consignes: Ouvrez un Terminal et d\u00e9placez-vous dans votre dossier de travail (avec la commande cd ). Tapez la commande permettant de r\u00e9cup\u00e9rer les fichiers n\u00e9cessaires \u00e0 ce TP. git clone https://gitlab.ec-lyon.fr/sderrode/s9_mod21_bigdata_tp.git Constatez, dans un gestionnaire de fichiers, que cette commande a permis de rapatrier des fichiers r\u00e9partis dans 2 dossiers : TP_Hadoop et TP_SparQL . Nous allons ici travailler sur le dossier TP_Hadoop . Dans le Terminal , d\u00e9placez-vous dans le dossier wordcount , en lan\u00e7ant successivement les 3 commandes suivantes : cd s9_mod21_bigdata_tp cd TP_Hadoop cd wordcount ou plus simplement : cd s9_mod21_bigdata_tp/TP_Hadoop/wordcount La commande ls permet de lister le contenu du dossier. Vous pouvez observer la pr\u00e9sence des 2 fichiers mapper.py et reducer.py , ainsi que du livre Dracula (libre de droit, t\u00e9l\u00e9charg\u00e9 depuis cette adresse ). Lancez la commande suivante et observez le r\u00e9sultat: cat dracula | python mapper.py Lancez ensuite la commande enti\u00e8re et observez le r\u00e9sultat: cat dracula | python mapper.py | sort | python reducer.py Remarque Windows Pour les \u00e9tudiants utilisant Windows , vous pourriez rencontrer des difficult\u00e9s avec les 2 lignes suivantes, qui font appel \u00e0 Python en ligne de commandes. La raison : l\u2019endroit o\u00f9 est stock\u00e9 le programme python.exe n\u2019est pas connu de votre machine, il faut donc le pr\u00e9ciser en modifiant la variable d\u2019environnement PATH . Pour cela, vous pouvez suivre les indications donn\u00e9es dans la Section Method 2: Manually add Python to Windows Path de ce lien . Exercice 1 - Am\u00e9lioration du wordcount \u00b6 Remarque : Pour stocker le r\u00e9sultat d\u2019ex\u00e9cution du script dans un fichier appel\u00e9 result.txt , on lancera cat dracula | python mapper.py | sort | python reducer.py > results.txt Ouvrez ce fichier avec votre \u00e9diteur de texte pr\u00e9f\u00e9r\u00e9, et regardez les premi\u00e8res lignes. On constate de nombreux probl\u00e8mes : la pr\u00e9sence de signes de ponctuation dans les mots; les mots commen\u00e7ant par une majuscule sont distingu\u00e9s des mots commen\u00e7ant par une minuscule. Pour r\u00e9gler ces probl\u00e8mes, veuillez Modifier les scripts Python pr\u00e9c\u00e9dents pour qu\u2019ils ne distinguent plus les mots qui comportent des majuscules et les m\u00eames mots qui n\u2019en comportent pas. Modifier la version pr\u00e9c\u00e9dente, de telle mani\u00e8re que les signes de ponctuation ne soient plus pris en compte (consultez Internet pour trouver un moyen de supprimer les signes de ponctuation d\u2019une cha\u00eene de caract\u00e8res repr\u00e9sentant un mot).","title":"Wordcount Local"},{"location":"TP2/Wordcount_Local/#wordcount-en-local","text":"Nous allons ici faire fonctionner l\u2019algorithme map-reduce qui compte les mots d\u2019un fichier texte, en local, i.e. sans exploiter le parall\u00e9lisme que propose le framework Hadoop . Le programme est constitu\u00e9 de deux scripts Python qui sont appel\u00e9s successivement selon la m\u00e9thode d\u00e9crite ci-dessous. Il s\u2019agit ici surtout de comprendre la logique algorithmique.","title":"Wordcount, en local"},{"location":"TP2/Wordcount_Local/#recuperation-et-lancement-des-scripts-python","text":"Pour r\u00e9cup\u00e9rer le scripts, suivez les consignes: Ouvrez un Terminal et d\u00e9placez-vous dans votre dossier de travail (avec la commande cd ). Tapez la commande permettant de r\u00e9cup\u00e9rer les fichiers n\u00e9cessaires \u00e0 ce TP. git clone https://gitlab.ec-lyon.fr/sderrode/s9_mod21_bigdata_tp.git Constatez, dans un gestionnaire de fichiers, que cette commande a permis de rapatrier des fichiers r\u00e9partis dans 2 dossiers : TP_Hadoop et TP_SparQL . Nous allons ici travailler sur le dossier TP_Hadoop . Dans le Terminal , d\u00e9placez-vous dans le dossier wordcount , en lan\u00e7ant successivement les 3 commandes suivantes : cd s9_mod21_bigdata_tp cd TP_Hadoop cd wordcount ou plus simplement : cd s9_mod21_bigdata_tp/TP_Hadoop/wordcount La commande ls permet de lister le contenu du dossier. Vous pouvez observer la pr\u00e9sence des 2 fichiers mapper.py et reducer.py , ainsi que du livre Dracula (libre de droit, t\u00e9l\u00e9charg\u00e9 depuis cette adresse ). Lancez la commande suivante et observez le r\u00e9sultat: cat dracula | python mapper.py Lancez ensuite la commande enti\u00e8re et observez le r\u00e9sultat: cat dracula | python mapper.py | sort | python reducer.py Remarque Windows Pour les \u00e9tudiants utilisant Windows , vous pourriez rencontrer des difficult\u00e9s avec les 2 lignes suivantes, qui font appel \u00e0 Python en ligne de commandes. La raison : l\u2019endroit o\u00f9 est stock\u00e9 le programme python.exe n\u2019est pas connu de votre machine, il faut donc le pr\u00e9ciser en modifiant la variable d\u2019environnement PATH . Pour cela, vous pouvez suivre les indications donn\u00e9es dans la Section Method 2: Manually add Python to Windows Path de ce lien .","title":"R\u00e9cup\u00e9ration et lancement des scripts Python"},{"location":"TP2/Wordcount_Local/#exercice-1-amelioration-du-wordcount","text":"Remarque : Pour stocker le r\u00e9sultat d\u2019ex\u00e9cution du script dans un fichier appel\u00e9 result.txt , on lancera cat dracula | python mapper.py | sort | python reducer.py > results.txt Ouvrez ce fichier avec votre \u00e9diteur de texte pr\u00e9f\u00e9r\u00e9, et regardez les premi\u00e8res lignes. On constate de nombreux probl\u00e8mes : la pr\u00e9sence de signes de ponctuation dans les mots; les mots commen\u00e7ant par une majuscule sont distingu\u00e9s des mots commen\u00e7ant par une minuscule. Pour r\u00e9gler ces probl\u00e8mes, veuillez Modifier les scripts Python pr\u00e9c\u00e9dents pour qu\u2019ils ne distinguent plus les mots qui comportent des majuscules et les m\u00eames mots qui n\u2019en comportent pas. Modifier la version pr\u00e9c\u00e9dente, de telle mani\u00e8re que les signes de ponctuation ne soient plus pris en compte (consultez Internet pour trouver un moyen de supprimer les signes de ponctuation d\u2019une cha\u00eene de caract\u00e8res repr\u00e9sentant un mot).","title":"Exercice 1 - Am\u00e9lioration du wordcount"},{"location":"TP3/","text":"TP #3 - Spark \u00b6 Ce TP fait suite au cours sur le framework Apache Spark , d\u00e9velopp\u00e9 et maintenu par la Fondation Apache . Partie 1 - Programmation fonctionnelle en Python \u00b6 Travail \u00e0 faire Programmez un algorithme qui calcule le nombre de secondes \u00e0 partir d\u2019une heure donn\u00e9e dans le format suivant : hh:mm:ss . Ainsi 8:19:22 donnera 29962 secondes. D\u00e9veloppez une premi\u00e8re version it\u00e9rative python structur\u00e9e , puis une version python fonctionnelle de l\u2019algorithme (utilisation des fonctions map(\u2026) et reduce(\u2026) ). Partie 2 - Tests de Spark , avec la librairie pyspark \u00b6 Pour lancer votre premier script pyspsark , suivez les consignes expos\u00e9es dans le fichier : Test_Spark.md Partie 3 - Programmez avec pyspark \u00b6 Le travail \u00e0 r\u00e9aliser durant la s\u00e9ance de TP est pr\u00e9sent\u00e9 dans le fichier : Enonce_TP_Spark.md Partie 4 - Spark streaming \u00b6 Spark streaming est une extension de la librairie principale de Spark , qui permet de traiter des flux continus de donn\u00e9es. Suivez les consignes d\u00e9crites dans ce fichier pour mettre en place un petite manip: wc_streaming.md Partie 5 - Spark MLib \u00b6 TO DO TO DO TO DO MLlib is Spark\u2019s machine learning (ML) library . Its goal is to make practical machine learning scalable and easy. At a high level, it provides tools such as: ML Algorithms: common learning algorithms such as classification, regression, clustering, and collaborative filtering Featurization: feature extraction, transformation, dimensionality reduction, and selection Pipelines: tools for constructing, evaluating, and tuning ML Pipelines Persistence: saving and load algorithms, models, and Pipelines Utilities: linear algebra, statistics, data handling, etc.","title":"TP3"},{"location":"TP3/#tp-3-spark","text":"Ce TP fait suite au cours sur le framework Apache Spark , d\u00e9velopp\u00e9 et maintenu par la Fondation Apache .","title":"TP #3 - Spark"},{"location":"TP3/#partie-1-programmation-fonctionnelle-en-python","text":"Travail \u00e0 faire Programmez un algorithme qui calcule le nombre de secondes \u00e0 partir d\u2019une heure donn\u00e9e dans le format suivant : hh:mm:ss . Ainsi 8:19:22 donnera 29962 secondes. D\u00e9veloppez une premi\u00e8re version it\u00e9rative python structur\u00e9e , puis une version python fonctionnelle de l\u2019algorithme (utilisation des fonctions map(\u2026) et reduce(\u2026) ).","title":"Partie 1 - Programmation fonctionnelle en Python"},{"location":"TP3/#partie-2-tests-de-spark-avec-la-librairie-pyspark","text":"Pour lancer votre premier script pyspsark , suivez les consignes expos\u00e9es dans le fichier : Test_Spark.md","title":"Partie 2 - Tests de Spark, avec la librairie pyspark"},{"location":"TP3/#partie-3-programmez-avec-pyspark","text":"Le travail \u00e0 r\u00e9aliser durant la s\u00e9ance de TP est pr\u00e9sent\u00e9 dans le fichier : Enonce_TP_Spark.md","title":"Partie 3 - Programmez avec pyspark"},{"location":"TP3/#partie-4-spark-streaming","text":"Spark streaming est une extension de la librairie principale de Spark , qui permet de traiter des flux continus de donn\u00e9es. Suivez les consignes d\u00e9crites dans ce fichier pour mettre en place un petite manip: wc_streaming.md","title":"Partie 4 - Spark streaming"},{"location":"TP3/#partie-5-spark-mlib","text":"TO DO TO DO TO DO MLlib is Spark\u2019s machine learning (ML) library . Its goal is to make practical machine learning scalable and easy. At a high level, it provides tools such as: ML Algorithms: common learning algorithms such as classification, regression, clustering, and collaborative filtering Featurization: feature extraction, transformation, dimensionality reduction, and selection Pipelines: tools for constructing, evaluating, and tuning ML Pipelines Persistence: saving and load algorithms, models, and Pipelines Utilities: linear algebra, statistics, data handling, etc.","title":"Partie 5 - Spark MLib"},{"location":"TP3/Enonce_TP_Spark/","text":"Sommaire TP avec Spark : Il est o\u00f9 le bel arbre ? TP avec Spark : Il est o\u00f9 le bel arbre ? \u00b6 On consid\u00e8re ici le fichier de donn\u00e9es Opendata Paris , de type CSV, concernant des arbres remarquables \u00e0 Paris. Ce fichier est disponible au t\u00e9l\u00e9chargement . Choisissez le menu Export et le format csv . Copiez ce fichier dans le Namenoe ( hadoop-master ). La commande more arbresremarquablesparis.csv montre que chaque ligne d\u00e9crit un arbre : position GPS, arrondissement, genre, esp\u00e8ce, famille, ann\u00e9e de plantation, hauteur, circonf\u00e9rence, etc. Le s\u00e9parateur entre les colonnes est le caract\u00e8re \u2018;\u2019. La premi\u00e8re ligne du fichier contient les titres des colonnes ; pour la supprimer, il suffit d\u2019\u00e9crire: sed '1d' arbresremarquablesparis.csv > arbresremarquablesparis2.csv Copiez alors arbresremarquablesparis2.csv sur HDFS (dans le r\u00e9pertoires input ). Remarque : Vous ne pouvez pas r\u00e9diger vos algo sur le Namenode (il n\u2019y a pas d\u2019\u00e9diteur de texte). \u00c9ditez le fichier sur votre syst\u00e8me d\u2019exploitation, et envoyez-le sur le Namenode par la commande Docker cp .. . Travail \u00e0 faire \u00c9crivez des programmes pyspark permettant d\u2019afficher les coordonn\u00e9es GPS, la taille et l\u2019adresse de l\u2019arbre le plus grand. Certains arbres n\u2019ont pas de taille renseign\u00e9e : on devra donc filtrer le RDD en supprimant les lignes dont la hauteur est \u00e9gale \u00e0 \u2018\u2019. La hauteur sera consid\u00e9r\u00e9e comme une cha\u00eene de caract\u00e8res. Pour la transformer en nombre r\u00e9el : float(hauteur) . Pour v\u00e9rifier le r\u00e9sultat, vous pouvez utiliser ce site . Entrez les coordonn\u00e9es GPS trouv\u00e9es et v\u00e9rifiez que l\u2019adresse est bien la bonne ! d\u2019afficher les coordonn\u00e9es GPS des arbres de plus grande circonf\u00e9rence pour chaque arrondissement de la ville de Paris. d\u2019afficher toutes les esp\u00e8ces d\u2019arbre, tri\u00e9es par genre.","title":"Enonce TP Spark"},{"location":"TP3/Enonce_TP_Spark/#tp-avec-spark-il-est-ou-le-bel-arbre","text":"On consid\u00e8re ici le fichier de donn\u00e9es Opendata Paris , de type CSV, concernant des arbres remarquables \u00e0 Paris. Ce fichier est disponible au t\u00e9l\u00e9chargement . Choisissez le menu Export et le format csv . Copiez ce fichier dans le Namenoe ( hadoop-master ). La commande more arbresremarquablesparis.csv montre que chaque ligne d\u00e9crit un arbre : position GPS, arrondissement, genre, esp\u00e8ce, famille, ann\u00e9e de plantation, hauteur, circonf\u00e9rence, etc. Le s\u00e9parateur entre les colonnes est le caract\u00e8re \u2018;\u2019. La premi\u00e8re ligne du fichier contient les titres des colonnes ; pour la supprimer, il suffit d\u2019\u00e9crire: sed '1d' arbresremarquablesparis.csv > arbresremarquablesparis2.csv Copiez alors arbresremarquablesparis2.csv sur HDFS (dans le r\u00e9pertoires input ). Remarque : Vous ne pouvez pas r\u00e9diger vos algo sur le Namenode (il n\u2019y a pas d\u2019\u00e9diteur de texte). \u00c9ditez le fichier sur votre syst\u00e8me d\u2019exploitation, et envoyez-le sur le Namenode par la commande Docker cp .. . Travail \u00e0 faire \u00c9crivez des programmes pyspark permettant d\u2019afficher les coordonn\u00e9es GPS, la taille et l\u2019adresse de l\u2019arbre le plus grand. Certains arbres n\u2019ont pas de taille renseign\u00e9e : on devra donc filtrer le RDD en supprimant les lignes dont la hauteur est \u00e9gale \u00e0 \u2018\u2019. La hauteur sera consid\u00e9r\u00e9e comme une cha\u00eene de caract\u00e8res. Pour la transformer en nombre r\u00e9el : float(hauteur) . Pour v\u00e9rifier le r\u00e9sultat, vous pouvez utiliser ce site . Entrez les coordonn\u00e9es GPS trouv\u00e9es et v\u00e9rifiez que l\u2019adresse est bien la bonne ! d\u2019afficher les coordonn\u00e9es GPS des arbres de plus grande circonf\u00e9rence pour chaque arrondissement de la ville de Paris. d\u2019afficher toutes les esp\u00e8ces d\u2019arbre, tri\u00e9es par genre.","title":"TP avec Spark : Il est o\u00f9 le bel arbre ?"},{"location":"TP3/Test_Spark/","text":"Sommaire Tests de Spark, avec la librairie pyspark Relancer le cluster wordcount en Spark Tester les scripts du cours Tests de Spark , avec la librairie pyspark \u00b6 Nous allons ici faire fonctionner l\u2019algorithme de comptage de mot, mais r\u00e9dig\u00e9 avec pyspsark , la librairie Python qui permet de programmer Spark . Relancer le cluster \u00b6 Il faut dans un premier temps, relancez le cluster que nous avions install\u00e9 pour Hadoop map-reduce , avec son Namenode et ses deux Datanodes . Tout d\u2019abord, lancer Docker Desktop . Puis, dans un premier Terminal , tapez : docker start hadoop-master hadoop-slave1 hadoop-slave2 Puis entrez dans le shell du Namenode : docker exec -it hadoop-master bash Lancez alors le daemon hadoop : ./start-hadoop.sh V\u00e9rifiez alors que HDFS est bien mont\u00e9, avec la commande : hadoop fs -ls wordcount en Spark \u00b6 Entrez dans le r\u00e9pertoire wordcount , listez les fichiers contenus dans ce r\u00e9pertoire : cd wordcount ls Il s\u2019agit des scripts python Hadoop du TP2 . Nous allons maintenant importer le m\u00eame programme mais r\u00e9dig\u00e9 en pyspark (ie. Spark pour Python ). Depuis un second Terminal , ouvert dans le r\u00e9pertoire o\u00f9 vous avez t\u00e9l\u00e9charg\u00e9 le contenu du r\u00e9pertoire scripts , tapez la commande docker cp PySpark_wc.py hadoop-master:/root/wordcount Revenez au premier Terminal , et v\u00e9rifiez que le fichier est l\u00e0 o\u00f9 il est attendu ! Avant de lancer le script, il convient de v\u00e9rifier que le r\u00e9pertoire sortie n\u2019existe pas d\u00e9j\u00e0 sous HDFS . Pour faire cela, on tente de l\u2019effacer (qu\u2019il existe ou non !) : hadoop fs -rm -r -f sortie et il convient de faire conna\u00eetre \u00e0 Spark la version de Python \u00e0 utiliser, \u00e0 travers une variable d\u2019environnement : export PYSPARK_PYTHON=python2.7 Ensuite, lancez le comptage de mots sur le livre dracula , en local, avec 2 c\u0153urs de votre processeur : spark-submit --master local[2] PySpark_wc.py input/dracula Pour v\u00e9rifier le r\u00e9sultat, scruter le contenu du r\u00e9pertoire sortie sou HDFS : dfs dfs \u2013ls sortie et le contenu des deux fichiers de sortie hdfs dfs \u2013text sortie/part-00000 hdfs dfs \u2013text sortie/part-00001 Travail \u00e0 faire Faites \u00e9voluer la version pr\u00e9c\u00e9dente de telle mani\u00e8re que l\u2019on ne garde que les mots qui apparaissent dans le texte au moins X fois, la valeur de X \u00e9tant fix\u00e9e par un argument suppl\u00e9mentaire lors de l\u2019appel \u00e0 spark-submit . Par exemple : spark-submit --master local[2] PySpark_wc.py input/dracula 1000 Tester les scripts du cours \u00b6 D\u2019abord, cr\u00e9ez et entrez dans un nouveau r\u00e9pertoire, \u00e0 la racine de votre compte : cd .. # pour remonter d'un niveau de r\u00e9pertoire dans l'arborescence mkdir pyspark cd pyspark Dans le second Terminal , rapatriez l\u2019ensemble des scripts PySpark_ex*.py dans le r\u00e9pertoire que nous venons de cr\u00e9er : for f in PySpark_ex*.py; do docker cp $f hadoop-master:/root/pyspark; done et rapatriez \u00e9galement le programme qui donne une approximation de pi : docker cp PySpark_Pi.py hadoop-master:/root/pyspark Remarque Pour le script PySpark_exemple5.py , vous aurez besoin du fichier baby_names_2013.csv , que j\u2019ai mis \u00e0 votre disposition dans le m\u00eame r\u00e9pertoire que ce fichier. N\u2019oubliez pas de le d\u00e9poser sur HDFS avant de lancer le script (vous savez comment faire maintenant\u2026).","title":"Test Spark"},{"location":"TP3/Test_Spark/#tests-de-spark-avec-la-librairie-pyspark","text":"Nous allons ici faire fonctionner l\u2019algorithme de comptage de mot, mais r\u00e9dig\u00e9 avec pyspsark , la librairie Python qui permet de programmer Spark .","title":"Tests de Spark, avec la librairie pyspark"},{"location":"TP3/Test_Spark/#relancer-le-cluster","text":"Il faut dans un premier temps, relancez le cluster que nous avions install\u00e9 pour Hadoop map-reduce , avec son Namenode et ses deux Datanodes . Tout d\u2019abord, lancer Docker Desktop . Puis, dans un premier Terminal , tapez : docker start hadoop-master hadoop-slave1 hadoop-slave2 Puis entrez dans le shell du Namenode : docker exec -it hadoop-master bash Lancez alors le daemon hadoop : ./start-hadoop.sh V\u00e9rifiez alors que HDFS est bien mont\u00e9, avec la commande : hadoop fs -ls","title":"Relancer le cluster"},{"location":"TP3/Test_Spark/#wordcount-en-spark","text":"Entrez dans le r\u00e9pertoire wordcount , listez les fichiers contenus dans ce r\u00e9pertoire : cd wordcount ls Il s\u2019agit des scripts python Hadoop du TP2 . Nous allons maintenant importer le m\u00eame programme mais r\u00e9dig\u00e9 en pyspark (ie. Spark pour Python ). Depuis un second Terminal , ouvert dans le r\u00e9pertoire o\u00f9 vous avez t\u00e9l\u00e9charg\u00e9 le contenu du r\u00e9pertoire scripts , tapez la commande docker cp PySpark_wc.py hadoop-master:/root/wordcount Revenez au premier Terminal , et v\u00e9rifiez que le fichier est l\u00e0 o\u00f9 il est attendu ! Avant de lancer le script, il convient de v\u00e9rifier que le r\u00e9pertoire sortie n\u2019existe pas d\u00e9j\u00e0 sous HDFS . Pour faire cela, on tente de l\u2019effacer (qu\u2019il existe ou non !) : hadoop fs -rm -r -f sortie et il convient de faire conna\u00eetre \u00e0 Spark la version de Python \u00e0 utiliser, \u00e0 travers une variable d\u2019environnement : export PYSPARK_PYTHON=python2.7 Ensuite, lancez le comptage de mots sur le livre dracula , en local, avec 2 c\u0153urs de votre processeur : spark-submit --master local[2] PySpark_wc.py input/dracula Pour v\u00e9rifier le r\u00e9sultat, scruter le contenu du r\u00e9pertoire sortie sou HDFS : dfs dfs \u2013ls sortie et le contenu des deux fichiers de sortie hdfs dfs \u2013text sortie/part-00000 hdfs dfs \u2013text sortie/part-00001 Travail \u00e0 faire Faites \u00e9voluer la version pr\u00e9c\u00e9dente de telle mani\u00e8re que l\u2019on ne garde que les mots qui apparaissent dans le texte au moins X fois, la valeur de X \u00e9tant fix\u00e9e par un argument suppl\u00e9mentaire lors de l\u2019appel \u00e0 spark-submit . Par exemple : spark-submit --master local[2] PySpark_wc.py input/dracula 1000","title":"wordcount en Spark"},{"location":"TP3/Test_Spark/#tester-les-scripts-du-cours","text":"D\u2019abord, cr\u00e9ez et entrez dans un nouveau r\u00e9pertoire, \u00e0 la racine de votre compte : cd .. # pour remonter d'un niveau de r\u00e9pertoire dans l'arborescence mkdir pyspark cd pyspark Dans le second Terminal , rapatriez l\u2019ensemble des scripts PySpark_ex*.py dans le r\u00e9pertoire que nous venons de cr\u00e9er : for f in PySpark_ex*.py; do docker cp $f hadoop-master:/root/pyspark; done et rapatriez \u00e9galement le programme qui donne une approximation de pi : docker cp PySpark_Pi.py hadoop-master:/root/pyspark Remarque Pour le script PySpark_exemple5.py , vous aurez besoin du fichier baby_names_2013.csv , que j\u2019ai mis \u00e0 votre disposition dans le m\u00eame r\u00e9pertoire que ce fichier. N\u2019oubliez pas de le d\u00e9poser sur HDFS avant de lancer le script (vous savez comment faire maintenant\u2026).","title":"Tester les scripts du cours"},{"location":"TP3/wc_streaming/","text":"Sommaire Spark streaming wordcount en streaming Spark streaming \u00b6 Spark streaming ( Spark streaming programming guide ) est une extension de la librairie principale de Spark , qui permet de traiter des flux continus de donn\u00e9es. Elle est tol\u00e9rante aux erreurs et permet de r\u00e9aliser des algorithmes complexes gr\u00e2ce \u00e0 des fonctions de haut niveau comme map , reduce , join , window . Finalement, les donn\u00e9es trait\u00e9es peuvent \u00eatre sauvegard\u00e9es sur diff\u00e9rents syst\u00e8mes de fichiers, dans des bases de donn\u00e9es ou dans des tableaux de bord interactifs. Vous pouvez \u00e9galement appliquer des algorithmes Spark de Machine Learning ou de traitement de graphes sur les flux de donn\u00e9es. Spark Streaming re\u00e7oit les flux de donn\u00e9es et les divise en paquets qui sont trait\u00e9s par le Spark Engine pour g\u00e9n\u00e9rer le r\u00e9sultat sous forme de paquets. La librairie fournit des objets appel\u00e9s DStream , pour Discretized Stream , qui repr\u00e9sente un flux continu de donn\u00e9es. Les DStream peuvent soit \u00eatre cr\u00e9\u00e9s par des flux de donn\u00e9es entrants (HDFS), soit par des sources provenant de Kafka , Flume ou Kinesis , ou encore par des op\u00e9rations sur des DStreams . En interne, un DStream est repr\u00e9sent\u00e9 par une s\u00e9quence de RDDs. wordcount en streaming \u00b6 Dans le Namenode , cr\u00e9ez un nouveau r\u00e9pertoire et d\u00e9placez-vous dedans : cd .. mkdir sparkstreaming cd sparkstreaming/ Depuis le second Terminal , copiez le fichier SparkStreaming_wc.py dans le Namenode : docker cp SparkStreaming_wc.py hadoop-master:/root/sparkstreaming Dans le premier Terminal , installez le serveur de donn\u00e9es netcat apt-get install netcat Ouvrez un troisi\u00e8me Terminal , et entrez dans le Namenode docker exec -it hadoop-master bash Ainsi, si vous suivez bien, les Terminaux 1 et 3 pointent tous deux sur le Namenode . Dans ce troisi\u00e8me Terminal , lancez le serveur de donn\u00e9es netcat sur le port 9999 de la mani\u00e8re suivante : nc -l -p 9999 Dans le premier Terminal , lancez spark-submit --master local[2] SparkStreaming_wc.py localhost 9999 Disposez les fen\u00eatres des Terminaux 1 et 3 c\u00f4te \u00e0 c\u00f4te. Sur le Terminal 3, tapez des mots rapidement, vous devriez voir le comptage de mot, chaque seconde, sur le Terminal 1. Remarques : pour entrer un s\u00e9rie de mots d\u2019un coup sur le Terminal 3, vous pouvez soit \u00e9crire une ligne de mots s\u00e9par\u00e9s par des espaces. Envoyez alors la ligne d\u2019un coup, en appuyant sur la touche entr\u00e9e de votre clavier. copier en m\u00e9moire ( CTRL+C ou CMD+C ) une ligne d\u2019un texte quelconque, et la coller ( CTRL+V ou CMD+V ) dans le Terminal 3. Les paquets de donn\u00e9es correspondent \u00e0 une dur\u00e9e d\u20181 seconde. Pour allonger le temps associ\u00e9 aux paquets de donn\u00e9es, il suffit de modifier la ligne ssc = StreamingContext(sc, 1) , en rempla\u00e7ant la valeur 1 par la valeur souhait\u00e9e en secondes.","title":"Wc streaming"},{"location":"TP3/wc_streaming/#spark-streaming","text":"Spark streaming ( Spark streaming programming guide ) est une extension de la librairie principale de Spark , qui permet de traiter des flux continus de donn\u00e9es. Elle est tol\u00e9rante aux erreurs et permet de r\u00e9aliser des algorithmes complexes gr\u00e2ce \u00e0 des fonctions de haut niveau comme map , reduce , join , window . Finalement, les donn\u00e9es trait\u00e9es peuvent \u00eatre sauvegard\u00e9es sur diff\u00e9rents syst\u00e8mes de fichiers, dans des bases de donn\u00e9es ou dans des tableaux de bord interactifs. Vous pouvez \u00e9galement appliquer des algorithmes Spark de Machine Learning ou de traitement de graphes sur les flux de donn\u00e9es. Spark Streaming re\u00e7oit les flux de donn\u00e9es et les divise en paquets qui sont trait\u00e9s par le Spark Engine pour g\u00e9n\u00e9rer le r\u00e9sultat sous forme de paquets. La librairie fournit des objets appel\u00e9s DStream , pour Discretized Stream , qui repr\u00e9sente un flux continu de donn\u00e9es. Les DStream peuvent soit \u00eatre cr\u00e9\u00e9s par des flux de donn\u00e9es entrants (HDFS), soit par des sources provenant de Kafka , Flume ou Kinesis , ou encore par des op\u00e9rations sur des DStreams . En interne, un DStream est repr\u00e9sent\u00e9 par une s\u00e9quence de RDDs.","title":"Spark streaming"},{"location":"TP3/wc_streaming/#wordcount-en-streaming","text":"Dans le Namenode , cr\u00e9ez un nouveau r\u00e9pertoire et d\u00e9placez-vous dedans : cd .. mkdir sparkstreaming cd sparkstreaming/ Depuis le second Terminal , copiez le fichier SparkStreaming_wc.py dans le Namenode : docker cp SparkStreaming_wc.py hadoop-master:/root/sparkstreaming Dans le premier Terminal , installez le serveur de donn\u00e9es netcat apt-get install netcat Ouvrez un troisi\u00e8me Terminal , et entrez dans le Namenode docker exec -it hadoop-master bash Ainsi, si vous suivez bien, les Terminaux 1 et 3 pointent tous deux sur le Namenode . Dans ce troisi\u00e8me Terminal , lancez le serveur de donn\u00e9es netcat sur le port 9999 de la mani\u00e8re suivante : nc -l -p 9999 Dans le premier Terminal , lancez spark-submit --master local[2] SparkStreaming_wc.py localhost 9999 Disposez les fen\u00eatres des Terminaux 1 et 3 c\u00f4te \u00e0 c\u00f4te. Sur le Terminal 3, tapez des mots rapidement, vous devriez voir le comptage de mot, chaque seconde, sur le Terminal 1. Remarques : pour entrer un s\u00e9rie de mots d\u2019un coup sur le Terminal 3, vous pouvez soit \u00e9crire une ligne de mots s\u00e9par\u00e9s par des espaces. Envoyez alors la ligne d\u2019un coup, en appuyant sur la touche entr\u00e9e de votre clavier. copier en m\u00e9moire ( CTRL+C ou CMD+C ) une ligne d\u2019un texte quelconque, et la coller ( CTRL+V ou CMD+V ) dans le Terminal 3. Les paquets de donn\u00e9es correspondent \u00e0 une dur\u00e9e d\u20181 seconde. Pour allonger le temps associ\u00e9 aux paquets de donn\u00e9es, il suffit de modifier la ligne ssc = StreamingContext(sc, 1) , en rempla\u00e7ant la valeur 1 par la valeur souhait\u00e9e en secondes.","title":"wordcount en streaming"},{"location":"Template_FS/","text":"Template - Fiche de synth\u00e8se \u00b6 Ce document pr\u00e9sente les consignes pour la r\u00e9daction de la fiche de synth\u00e8se (FS), \u00e0 rendre avant le jour de l\u2019examen \u00e9crit. Consignes \u00b6 Votre FS doit \u00eatre r\u00e9dig\u00e9e avec le format Markdown et suivre le mod\u00e8le pr\u00e9sent\u00e9 ci-dessous. Elle ne doit pas d\u00e9passer l\u2019\u00e9quivalent d\u2019une dizaine de pages. Le contenu de la FS doit \u00eatre g\u00e9r\u00e9 entre vous, avec l\u2019aide d\u2019un projet GitHub . Le projet sera h\u00e9berg\u00e9 sur le compte de l\u2019un des membres du groupe, qui autorisera l\u2019acc\u00e8s en \u00e9criture au projet \u00e0 tous les autres membres du projet ainsi qu\u2019\u00e0 moi-m\u00eame (mon pseudo GitHub : SDerrode ). Le groupe doit se d\u00e9clarer sur ce fichier partag\u00e9 , qui contient autant d\u2019obglet qu\u2019il y a de groupes. On y d\u00e9posera le titre de la fiche, l\u2019adresse URL du projet GitHub et les noms et pseudo des \u00e9tudiants. Utilisez le premier onglet vide disponible (si n\u00e9cessaire, ajoutez des onglets). Je r\u00e9cup\u00e9rerai votre FS directement sur votre projet GitHub le jour de l\u2019examen. Le jour de l\u2019examen constitue donc la date butoir pour terminer votre FS. Remarque Le syst\u00e8me de gestion de versions git est pr\u00e9sent\u00e9 en cours. Vous trouverez le scenario du tuto jou\u00e9 durant le second cours dans le r\u00e9pertoire tuto-git-gitlab de ce projet GitHub . Markdown \u00b6 Markdown est un simple format texte dans lequel on ajoute des balises, pour mettre en gras, inclure des images, faire des liens hypertextes\u2026 Ces balises peuvent \u00eatre interpr\u00e9t\u00e9es par diff\u00e9rents moteurs pour obtenir un rendu en html , en pdf , en rtf ou en word \u2026 Pour apprendre les quelques balises n\u00e9cessaires \u00e0 une mise en page minimaliste (mais suffisante la plupart du temps), vous trouverez de nombreux tutoriaux sur internet, dont celui-ci : Markdown - Wikip\u00e9dia . La totalit\u00e9 des \u00e9diteurs de textes g\u00e9n\u00e9ralistes, tels Sublime Text , Geany \u2026, g\u00e8rent tr\u00e8s bien ce format, et incluent des convertisseurs automatiques. Remarquez d\u2019ailleurs que GitHub interpr\u00e8tent automatiquement les fichiers Markdown que vous d\u00e9posez sur leur site. Voici cependant quelques outils open source sp\u00e9cifiques qui pourraient vous aider pour d\u00e9buter: ghostwriter : Linux et Windows abricotine : Linux , Windows et Mac OS X (en version beta) remarkable : Windows et Linux Il existe des \u00e9diteurs en ligne, dont Dilinger , StackEdit (ce dernier requiert une inscription). Organisation de la FS \u00b6 La FS sera compos\u00e9e de plusieurs fichiers Markdown . Le fichier principal de votre FS sera appel\u00e9 README.md , et inclura des liens vers les diff\u00e9rents fichiers qui composent votre fiche. Votre fichier README.md sera organis\u00e9 de la mani\u00e8re suivante : le titre de votre FS, les noms des auteurs, l\u2019ann\u00e9e (les informations classiques d\u2019une page de garde). un r\u00e9sum\u00e9 d\u2019une dizaine de lignes. le sommaire de votre document. des liens vers des fichiers Markdown qui d\u00e9veloppent votre sujet d\u2019\u00e9tude (Introduction, chapitre 1, chapitre 2, \u2026, Conclusion). un lien vers un fichier Markdown qui contiendra la bibliographie et la webographie. des liens vers des fichiers Markdown qui pr\u00e9sentent d\u2019\u00e9ventuelles annexes. Ajouts d\u2019images, de sch\u00e9mas ou de diagrammes \u00b6 Si vous souhaitez int\u00e9grer des diagrammes ou des sch\u00e9ma (ou tout autre diagramme vectoriel), je vous conseille l\u2019utilisation de diagram.net (open source). Vous pouvez sauvegarder vos graphiques diagram.net sur votre machine (cf exemple dans le dossier figures , \u00e0 c\u00f4t\u00e9 de ce fichier) et les convertir dans des images aux formats png , svg \u2026 pour les int\u00e9grer ensuite dans Markdown selon l\u2019exemple suivant : la commande ![test](figures/test.png) donne la commande ![test](figures/test.svg) donne","title":"Template_FS"},{"location":"Template_FS/#template-fiche-de-synthese","text":"Ce document pr\u00e9sente les consignes pour la r\u00e9daction de la fiche de synth\u00e8se (FS), \u00e0 rendre avant le jour de l\u2019examen \u00e9crit.","title":"Template - Fiche de synth\u00e8se"},{"location":"Template_FS/#consignes","text":"Votre FS doit \u00eatre r\u00e9dig\u00e9e avec le format Markdown et suivre le mod\u00e8le pr\u00e9sent\u00e9 ci-dessous. Elle ne doit pas d\u00e9passer l\u2019\u00e9quivalent d\u2019une dizaine de pages. Le contenu de la FS doit \u00eatre g\u00e9r\u00e9 entre vous, avec l\u2019aide d\u2019un projet GitHub . Le projet sera h\u00e9berg\u00e9 sur le compte de l\u2019un des membres du groupe, qui autorisera l\u2019acc\u00e8s en \u00e9criture au projet \u00e0 tous les autres membres du projet ainsi qu\u2019\u00e0 moi-m\u00eame (mon pseudo GitHub : SDerrode ). Le groupe doit se d\u00e9clarer sur ce fichier partag\u00e9 , qui contient autant d\u2019obglet qu\u2019il y a de groupes. On y d\u00e9posera le titre de la fiche, l\u2019adresse URL du projet GitHub et les noms et pseudo des \u00e9tudiants. Utilisez le premier onglet vide disponible (si n\u00e9cessaire, ajoutez des onglets). Je r\u00e9cup\u00e9rerai votre FS directement sur votre projet GitHub le jour de l\u2019examen. Le jour de l\u2019examen constitue donc la date butoir pour terminer votre FS. Remarque Le syst\u00e8me de gestion de versions git est pr\u00e9sent\u00e9 en cours. Vous trouverez le scenario du tuto jou\u00e9 durant le second cours dans le r\u00e9pertoire tuto-git-gitlab de ce projet GitHub .","title":"Consignes"},{"location":"Template_FS/#markdown","text":"Markdown est un simple format texte dans lequel on ajoute des balises, pour mettre en gras, inclure des images, faire des liens hypertextes\u2026 Ces balises peuvent \u00eatre interpr\u00e9t\u00e9es par diff\u00e9rents moteurs pour obtenir un rendu en html , en pdf , en rtf ou en word \u2026 Pour apprendre les quelques balises n\u00e9cessaires \u00e0 une mise en page minimaliste (mais suffisante la plupart du temps), vous trouverez de nombreux tutoriaux sur internet, dont celui-ci : Markdown - Wikip\u00e9dia . La totalit\u00e9 des \u00e9diteurs de textes g\u00e9n\u00e9ralistes, tels Sublime Text , Geany \u2026, g\u00e8rent tr\u00e8s bien ce format, et incluent des convertisseurs automatiques. Remarquez d\u2019ailleurs que GitHub interpr\u00e8tent automatiquement les fichiers Markdown que vous d\u00e9posez sur leur site. Voici cependant quelques outils open source sp\u00e9cifiques qui pourraient vous aider pour d\u00e9buter: ghostwriter : Linux et Windows abricotine : Linux , Windows et Mac OS X (en version beta) remarkable : Windows et Linux Il existe des \u00e9diteurs en ligne, dont Dilinger , StackEdit (ce dernier requiert une inscription).","title":"Markdown"},{"location":"Template_FS/#organisation-de-la-fs","text":"La FS sera compos\u00e9e de plusieurs fichiers Markdown . Le fichier principal de votre FS sera appel\u00e9 README.md , et inclura des liens vers les diff\u00e9rents fichiers qui composent votre fiche. Votre fichier README.md sera organis\u00e9 de la mani\u00e8re suivante : le titre de votre FS, les noms des auteurs, l\u2019ann\u00e9e (les informations classiques d\u2019une page de garde). un r\u00e9sum\u00e9 d\u2019une dizaine de lignes. le sommaire de votre document. des liens vers des fichiers Markdown qui d\u00e9veloppent votre sujet d\u2019\u00e9tude (Introduction, chapitre 1, chapitre 2, \u2026, Conclusion). un lien vers un fichier Markdown qui contiendra la bibliographie et la webographie. des liens vers des fichiers Markdown qui pr\u00e9sentent d\u2019\u00e9ventuelles annexes.","title":"Organisation de la FS"},{"location":"Template_FS/#ajouts-dimages-de-schemas-ou-de-diagrammes","text":"Si vous souhaitez int\u00e9grer des diagrammes ou des sch\u00e9ma (ou tout autre diagramme vectoriel), je vous conseille l\u2019utilisation de diagram.net (open source). Vous pouvez sauvegarder vos graphiques diagram.net sur votre machine (cf exemple dans le dossier figures , \u00e0 c\u00f4t\u00e9 de ce fichier) et les convertir dans des images aux formats png , svg \u2026 pour les int\u00e9grer ensuite dans Markdown selon l\u2019exemple suivant : la commande ![test](figures/test.png) donne la commande ![test](figures/test.svg) donne","title":"Ajouts d'images, de sch\u00e9mas ou de diagrammes"},{"location":"tuto-git-gitlab/git-commande/","text":"Tuto des principales commandes git \u00b6 Ce tuto pr\u00e9sente les principales commandes git , lorsqu\u2019on souhaite l\u2019utiliser en mode Terminal (\u00e0 l\u2019aide de commandes manuelles). Remarque :Vous devez savoir ouvrir un Terminal sur votre machine, quelque soit le syst\u00e8me d\u2019exploitation ( Windows , Linux , ou Mac OS X ). Sous Windows 10 (et ant\u00e9rieur), vous pourrez utiliser le programme Windows powershell qui est tr\u00e8s similaires au Terminal de Linux et de Mac OS X . Vous devez aussi savoir a minima naviguer dans vos dossiers \u00e0 l\u2019aide de la commande cd . Typiquement : cd c:\\Users\\stephane\\TP_Hadoop # Windows cd ~\\stephane\\TP_Hadoop # Mac, linux Si vous souhaitez remonter d\u2019un niveau dans la hi\u00e9rarchie des dossiers: cd .. . Des tutos vid\u00e9o existent pour d\u00e9coruvrir les comandes de bases (identiques \u00e0 celles que l\u2019on retourve sur les syst\u00e8mes Linux ). Configuration de git (\u00e0 faire une fois pour toute) >> git config --global color.ui true >> git config --global user.name \"votre_pseudo\" >> git config --global user.email moi@email.com >> git config --global core.editor geany #(for Linux if installed) >> git config --global core.editor \"/App.../Sublime\\ Text.app/.../bin/subl -w\" #(for Mac if installed) >> git config --global core.autocrlf true #(for Windows) >> git config --list >> cat ~/.gitconfig git clone / status / log >> git clone https://github.com/SDerrode/Test.git >> cd Test >> git status >> git log Il s\u2019agit ici du clonage, sur votre machine, d\u2019un projet existant stock\u00e9 sur gitlab ou github . Modification d\u2019un fichier existant >> subl readme.md #(modification du fichier) >> git status >> git commit -a -m \"modification de readme.md\" >> git status >> git log Cr\u00e9ation d\u2019un fichier et ajout \u00e0 git >> touch prog.py #(cr\u00e9ation d''un nouveau fichier vide) >> subl prog.py #(\u00e9criture d''un programme dans le fichier) >> python prog.py >> git status >> git add prog.py #(suivi du fichier par git) >> git add -A #(suivi de tous les nouveaux fichiers ) >> git commit -a -m \"ajout d'un fichier\" >> git status >> git log On peut utiliser ce proc\u00e9d\u00e9 pour ajouter un fichier gitignore sur la racine du projet. Diff\u00e9rence entre 2 commits >> git diff HEAD^ HEAD README.md #(entre l''avant dernier et le dernier commit) On peut remplacer HEAD^ et HEAD par deux num\u00e9ros de commit. Revenir en arri\u00e8re >> git reset --hard 82f5 #(attention suppression des commit les plus r\u00e9cents!) >> git checkout 1ce87820b4b1 #(bref saut dans un commit pr\u00e9c\u00e9dent) >> git log >> ls >> git checkout master (on revient dans l''\u00e9tat initial) >> git log >> ls Publication vers gitlab/github et mise \u00e0 jour depuis gitlab/github >> git status >> git push #(publication vers gitlab/github de tous les nouveaux commits) >> git pull #(r\u00e9cup\u00e9ration des changements publi\u00e9s par d'autres) Si un coll\u00e8gue du projet \u00e0 mis \u00e0 jour le d\u00e9p\u00f4t gitlab/github , alors il faut fusionner ces modifications avec vos propres travaux, localement sur votre machine. Avant de lancer la commande, pensez \u00e0 commiter vos propres changements. Si la fusion pose probl\u00e8mes (vous avez travaill\u00e9 sur le m\u00eame bout de code et vos corrections ne sont pas fusionable de mani\u00e8re automatique), alors git vos informe de la proc\u00e9dure manuelle \u00e0 suivre. Cr\u00e9ation d\u2019une branche Les branches sont essentiellement utilis\u00e9es pour tester une id\u00e9e sans perturber le projet. Vous pouvez g\u00e9n\u00e9rer des commits sur la branche, ind\u00e9pendamment de la branche principale. Si vous validez cette id\u00e9e, alors il devient possible de fusionner la branche avec la branche principale: >> git branch essai #(on cr\u00e9\u00e9 une nouvelle branche essai >> git branch >> git checkout essai #(on bascule vers cette branche) >> touch ALIRE.tx >> subl ALIRE.tx >> git add ALIRE.tx >> git commit -a -m \"add file ALIRE.tx\" >> ls >> git log >> git checkout master >> ls >> git merge essai -m \"merge de la branche essai\" #(vous pouvez pr\u00e9ciser un message qui explique le merge) >> ls >> git branch -d essai #(destruction de la branche apr\u00e8s merge) On peut tr\u00e8s bien merger une branche et continuer son d\u00e9veloppement pour faire un second merge plus tard. On peut aussi publier la branche sur gitlab/github (pour l\u2019instant elle ne reste que locale). Tagger une version >> git tag -a v0.1 HEAD -m \"my version 0.1\" >> git tag #(liste les tags) >> git push --tags #(envoi des tag sur le d\u00e9p\u00f4t gitlab/github) Archiver une version La commande suivante permet de g\u00e9n\u00e9rer un fichier compress\u00e9 comprenant l\u2019\u00e9tat actuel de votre projet (il ne contient pas les fichiers sp\u00e9cifique au suivi par git , tels le r\u00e9pertoire .git ou le fichier .gitignore ). >> git archive --format=tgz --prefix=git-0.1/ HEAD >git-0.1.tgz >> ls","title":"Git commande"},{"location":"tuto-git-gitlab/git-commande/#tuto-des-principales-commandes-git","text":"Ce tuto pr\u00e9sente les principales commandes git , lorsqu\u2019on souhaite l\u2019utiliser en mode Terminal (\u00e0 l\u2019aide de commandes manuelles). Remarque :Vous devez savoir ouvrir un Terminal sur votre machine, quelque soit le syst\u00e8me d\u2019exploitation ( Windows , Linux , ou Mac OS X ). Sous Windows 10 (et ant\u00e9rieur), vous pourrez utiliser le programme Windows powershell qui est tr\u00e8s similaires au Terminal de Linux et de Mac OS X . Vous devez aussi savoir a minima naviguer dans vos dossiers \u00e0 l\u2019aide de la commande cd . Typiquement : cd c:\\Users\\stephane\\TP_Hadoop # Windows cd ~\\stephane\\TP_Hadoop # Mac, linux Si vous souhaitez remonter d\u2019un niveau dans la hi\u00e9rarchie des dossiers: cd .. . Des tutos vid\u00e9o existent pour d\u00e9coruvrir les comandes de bases (identiques \u00e0 celles que l\u2019on retourve sur les syst\u00e8mes Linux ). Configuration de git (\u00e0 faire une fois pour toute) >> git config --global color.ui true >> git config --global user.name \"votre_pseudo\" >> git config --global user.email moi@email.com >> git config --global core.editor geany #(for Linux if installed) >> git config --global core.editor \"/App.../Sublime\\ Text.app/.../bin/subl -w\" #(for Mac if installed) >> git config --global core.autocrlf true #(for Windows) >> git config --list >> cat ~/.gitconfig git clone / status / log >> git clone https://github.com/SDerrode/Test.git >> cd Test >> git status >> git log Il s\u2019agit ici du clonage, sur votre machine, d\u2019un projet existant stock\u00e9 sur gitlab ou github . Modification d\u2019un fichier existant >> subl readme.md #(modification du fichier) >> git status >> git commit -a -m \"modification de readme.md\" >> git status >> git log Cr\u00e9ation d\u2019un fichier et ajout \u00e0 git >> touch prog.py #(cr\u00e9ation d''un nouveau fichier vide) >> subl prog.py #(\u00e9criture d''un programme dans le fichier) >> python prog.py >> git status >> git add prog.py #(suivi du fichier par git) >> git add -A #(suivi de tous les nouveaux fichiers ) >> git commit -a -m \"ajout d'un fichier\" >> git status >> git log On peut utiliser ce proc\u00e9d\u00e9 pour ajouter un fichier gitignore sur la racine du projet. Diff\u00e9rence entre 2 commits >> git diff HEAD^ HEAD README.md #(entre l''avant dernier et le dernier commit) On peut remplacer HEAD^ et HEAD par deux num\u00e9ros de commit. Revenir en arri\u00e8re >> git reset --hard 82f5 #(attention suppression des commit les plus r\u00e9cents!) >> git checkout 1ce87820b4b1 #(bref saut dans un commit pr\u00e9c\u00e9dent) >> git log >> ls >> git checkout master (on revient dans l''\u00e9tat initial) >> git log >> ls Publication vers gitlab/github et mise \u00e0 jour depuis gitlab/github >> git status >> git push #(publication vers gitlab/github de tous les nouveaux commits) >> git pull #(r\u00e9cup\u00e9ration des changements publi\u00e9s par d'autres) Si un coll\u00e8gue du projet \u00e0 mis \u00e0 jour le d\u00e9p\u00f4t gitlab/github , alors il faut fusionner ces modifications avec vos propres travaux, localement sur votre machine. Avant de lancer la commande, pensez \u00e0 commiter vos propres changements. Si la fusion pose probl\u00e8mes (vous avez travaill\u00e9 sur le m\u00eame bout de code et vos corrections ne sont pas fusionable de mani\u00e8re automatique), alors git vos informe de la proc\u00e9dure manuelle \u00e0 suivre. Cr\u00e9ation d\u2019une branche Les branches sont essentiellement utilis\u00e9es pour tester une id\u00e9e sans perturber le projet. Vous pouvez g\u00e9n\u00e9rer des commits sur la branche, ind\u00e9pendamment de la branche principale. Si vous validez cette id\u00e9e, alors il devient possible de fusionner la branche avec la branche principale: >> git branch essai #(on cr\u00e9\u00e9 une nouvelle branche essai >> git branch >> git checkout essai #(on bascule vers cette branche) >> touch ALIRE.tx >> subl ALIRE.tx >> git add ALIRE.tx >> git commit -a -m \"add file ALIRE.tx\" >> ls >> git log >> git checkout master >> ls >> git merge essai -m \"merge de la branche essai\" #(vous pouvez pr\u00e9ciser un message qui explique le merge) >> ls >> git branch -d essai #(destruction de la branche apr\u00e8s merge) On peut tr\u00e8s bien merger une branche et continuer son d\u00e9veloppement pour faire un second merge plus tard. On peut aussi publier la branche sur gitlab/github (pour l\u2019instant elle ne reste que locale). Tagger une version >> git tag -a v0.1 HEAD -m \"my version 0.1\" >> git tag #(liste les tags) >> git push --tags #(envoi des tag sur le d\u00e9p\u00f4t gitlab/github) Archiver une version La commande suivante permet de g\u00e9n\u00e9rer un fichier compress\u00e9 comprenant l\u2019\u00e9tat actuel de votre projet (il ne contient pas les fichiers sp\u00e9cifique au suivi par git , tels le r\u00e9pertoire .git ou le fichier .gitignore ). >> git archive --format=tgz --prefix=git-0.1/ HEAD >git-0.1.tgz >> ls","title":"Tuto des principales commandes git"},{"location":"tuto-git-gitlab/tuto-git-gitlab/","text":"Tuto sur git/gitlab \u00b6 Ce document pr\u00e9sente un tuto \u00e0 l\u2019usage de git / gitlab . Il est consultable sous forme de vid\u00e9o sur le Moodle du cours, \u00e0 l\u2019adresse https://pedagogie1.ec-lyon.fr/course/view.php?id=969. Comme tout tutoriel, ce document a besoin de vos retours pour s\u2019am\u00e9liorer! N\u2019h\u00e9sitez pas \u00e0 m\u2019envoyez un mail d\u00e9crivant vos difficult\u00e9s et, \u00e9ventuellement, les solutions que vous auriez trouv\u00e9es pour les contourner. Installer git et Github Desktop \u00b6 git est disponible par d\u00e9faut sur les machines Mac OS X et Linux . S\u2019il est absent de Windows (pour le savoir, lancez la commande git dans un Terminal/Invite de commandes), alors installez-le \u00e0 partir de ce lien : git-scm . Lors de l\u2019installation, validez les choix par d\u00e9faut qui vous sont propos\u00e9s. L\u2019interface Github Desktop que nous allons installer n\u2019est absolument pas n\u00e9cessaire pour travailler avec git. Mais elle \u00e9vite, dans un premier temps, d\u2019utiliser des commandes manuelles \u00e0 partir d\u2019un Terminal. T\u00e9l\u00e9chargez, installez et lancez l\u2019application Github Desktop . Configurer Github Desktop (cette \u00e9tape n\u2019est n\u00e9cessaire que si vous ne l\u2019avez pas d\u00e9j\u00e0 fait lors de l\u2019installation du logiciel) : aller dans le menu Pr\u00e9f\u00e9rences / git entrez votre nom et votre adresse \u00e9mail. G\u00e9rer son premier projet git avec gitlab \u00b6 Montrer l\u2019interface de gitlab Adresse: https://gitlab.ec-lyon.fr; Identifiant: ceux de Centrale; Naviguer dans le projet INF-TC2 (que les \u00e9tudiants ont d\u00e9j\u00e0 \u00e9tudi\u00e9). Donner quelques explications sur l\u2019interface. Cr\u00e9er un nouveau projet Nom : HelloWorld; Description: Mon premier projet gitlab; Discuter Public/Internal/Private ; S\u00e9lectionner Initialize repository with a README ; Cliquer sur l\u2019ic\u00f4ne Clone et copiez le lien Clone with HTTPS , qui ressemble \u00e0 https://gitlab.ec-lyon.fr/xyyyyy/helloworld.git. Basculer sur Github Desktop , S\u00e9lectionner l\u2019option Clone a Repository from the Internet , puis l\u2019onglet URL (ou menu File , Clone repository ). Copier l\u2019adresse pr\u00e9c\u00e9demment mise en m\u00e9moire dans l\u2019espace d\u00e9di\u00e9, et choisir dans local path le r\u00e9pertoire dans lequel votre projet sera copi\u00e9. Appuyez sur le bouton Clone de l\u2019interface pour lancer l\u2019importation de votre projet gitlab sur votre machine. V\u00e9rifier dans un gestionnaire de fichiers que le repo est bien copi\u00e9 localement sur sa machine et qu\u2019il contient un r\u00e9pertoire cach\u00e9 ( .git ) et le fichier readme.md . Basculer sur l\u2019onglet history Pour \u00e9viter que certains fichiers temporaires ne soient suivis par git , on va sp\u00e9cifier les fichiers a exclure dans un fichier appel\u00e9 .gitignore (il s\u2019agit d\u2019un fichier cach\u00e9). Pour cela Aller dans le menu Repository , sous-menu Repository settings , puis Ignored Files . Copier le contenu du fichier qui se trouve \u00e0 l\u2019adresse https://github.com/github/gitignore/blob/master/Python.gitignore dans l\u2019espace r\u00e9serv\u00e9, puis Save . On doit alors Commiter les changements (le texte par d\u00e9faut qui d\u00e9crit le commit est OK). Plus de d\u00e9tails sur ces commit plus loin. \u00c9diter le fichier readme.md avec Sublime Text et enregistrer les changements localement: Basculer vers Github Desktop et discuter les changements qui interviennent sur l\u2019interface. Commiter les changements, et monter l\u2019onglet History . Pusher le travail vers gitlab en utilisant l\u2019option Publish Branch . Basculer vers gitlab et v\u00e9rifier que les changements ont bien \u00e9taient publi\u00e9s Basculer vers Spyder pour \u00e9diter du code python: Taper un programme incremente_de_un(x) et sauvegarder dans votre r\u00e9pertoire local sous le nom de fichier incremente.py . commiter les changements dans Github Desktop Refaire la m\u00eame chose avec un autre fichier ( incremente_de_deux(x) ), \u00e0 sauvegarder dans un sous r\u00e9pertoire test . Pusher son travail vers gitlab , et v\u00e9rifier que tout se met bien \u00e0 jour. De mani\u00e8re synth\u00e9tique, une fois le projet cr\u00e9\u00e9, vous modifiez vos code sources, et enregistrez r\u00e9guli\u00e8rement vos changements avec un commit (donc en local). Vous pouvez commiter des sources m\u00eame s\u2019ils ne sont pas termin\u00e9s ou comportent des bugs. \u00c0 ce stade cela n\u2019a pas d\u2019importance. quand vous \u00eates content de votre travail, et que vos jugez vos algorithmes fonctionnels, vous publiez vos pr\u00e9c\u00e9dents commit sur gitlab avec un push . Ce comportement est illustr\u00e9 par le sch\u00e9ma de la figure suivante: Pour d\u00e9truire ce projet (sans grand inter\u00eat), il faut : Quitter Spyder pour farmer les fichiers en cours. D\u00e9truire la version locale en utilisant Github Desktop : s\u00e9lectionner l\u2019onglet Current Repository , et le menu contextuel sur le nom de votre projet permet d\u2019acc\u00e9der \u00e0 Remove . Penser \u00e0 cocher la suppression physique des fichiers si vous le souhaiter (sinon seul la gestion du projet par Github Desktop sera supprim\u00e9e, les fichiers seront toujorus pr\u00e9sents sur votre disque dur). D\u00e9truire la version distante sur gitlab : Dans la barre de menu lat\u00e9rale \u00e0 gauche de l\u2019interface Web, s\u00e9lectionnez l\u2019ic\u00f4ne repr\u00e9sentant une roue crant\u00e9e correspondant \u00e0 Settings . Dans le sous-menu G\u00e9n\u00e9ral , appuyez sur le bouton Expand correspondant \u00e0 la ligne : Advanced . Tout en bas, s\u00e9lectionner Delete Project , copier la phrase demand\u00e9e et appuyer sur le bouton pour confirmer. A quoi \u00e7a sert d\u2019utiliser git et gitlab ? Les avantages sont nombreux: Avoir un copie de son travail sur un serveur distant. Sauvegarde incr\u00e9mentale et dun historique des modification (undo infini!) Partager son travail avec tout le monde (si votre projet est publique), ou avec des personnes choisies (invitations personnelles). Travailler \u00e0 plusieurs sur le m\u00eame projet et en m\u00eame temps (mais \u00e7a, c\u2019est un autre tuto\u2026) fusion automatique (ou presque) des sources tra\u00e7age des modification (qui, qoui) St\u00e9phane Derrode","title":"Tuto git gitlab"},{"location":"tuto-git-gitlab/tuto-git-gitlab/#tuto-sur-gitgitlab","text":"Ce document pr\u00e9sente un tuto \u00e0 l\u2019usage de git / gitlab . Il est consultable sous forme de vid\u00e9o sur le Moodle du cours, \u00e0 l\u2019adresse https://pedagogie1.ec-lyon.fr/course/view.php?id=969. Comme tout tutoriel, ce document a besoin de vos retours pour s\u2019am\u00e9liorer! N\u2019h\u00e9sitez pas \u00e0 m\u2019envoyez un mail d\u00e9crivant vos difficult\u00e9s et, \u00e9ventuellement, les solutions que vous auriez trouv\u00e9es pour les contourner.","title":"Tuto sur git/gitlab"},{"location":"tuto-git-gitlab/tuto-git-gitlab/#installer-git-et-github-desktop","text":"git est disponible par d\u00e9faut sur les machines Mac OS X et Linux . S\u2019il est absent de Windows (pour le savoir, lancez la commande git dans un Terminal/Invite de commandes), alors installez-le \u00e0 partir de ce lien : git-scm . Lors de l\u2019installation, validez les choix par d\u00e9faut qui vous sont propos\u00e9s. L\u2019interface Github Desktop que nous allons installer n\u2019est absolument pas n\u00e9cessaire pour travailler avec git. Mais elle \u00e9vite, dans un premier temps, d\u2019utiliser des commandes manuelles \u00e0 partir d\u2019un Terminal. T\u00e9l\u00e9chargez, installez et lancez l\u2019application Github Desktop . Configurer Github Desktop (cette \u00e9tape n\u2019est n\u00e9cessaire que si vous ne l\u2019avez pas d\u00e9j\u00e0 fait lors de l\u2019installation du logiciel) : aller dans le menu Pr\u00e9f\u00e9rences / git entrez votre nom et votre adresse \u00e9mail.","title":"Installer git et Github Desktop"},{"location":"tuto-git-gitlab/tuto-git-gitlab/#gerer-son-premier-projet-git-avec-gitlab","text":"Montrer l\u2019interface de gitlab Adresse: https://gitlab.ec-lyon.fr; Identifiant: ceux de Centrale; Naviguer dans le projet INF-TC2 (que les \u00e9tudiants ont d\u00e9j\u00e0 \u00e9tudi\u00e9). Donner quelques explications sur l\u2019interface. Cr\u00e9er un nouveau projet Nom : HelloWorld; Description: Mon premier projet gitlab; Discuter Public/Internal/Private ; S\u00e9lectionner Initialize repository with a README ; Cliquer sur l\u2019ic\u00f4ne Clone et copiez le lien Clone with HTTPS , qui ressemble \u00e0 https://gitlab.ec-lyon.fr/xyyyyy/helloworld.git. Basculer sur Github Desktop , S\u00e9lectionner l\u2019option Clone a Repository from the Internet , puis l\u2019onglet URL (ou menu File , Clone repository ). Copier l\u2019adresse pr\u00e9c\u00e9demment mise en m\u00e9moire dans l\u2019espace d\u00e9di\u00e9, et choisir dans local path le r\u00e9pertoire dans lequel votre projet sera copi\u00e9. Appuyez sur le bouton Clone de l\u2019interface pour lancer l\u2019importation de votre projet gitlab sur votre machine. V\u00e9rifier dans un gestionnaire de fichiers que le repo est bien copi\u00e9 localement sur sa machine et qu\u2019il contient un r\u00e9pertoire cach\u00e9 ( .git ) et le fichier readme.md . Basculer sur l\u2019onglet history Pour \u00e9viter que certains fichiers temporaires ne soient suivis par git , on va sp\u00e9cifier les fichiers a exclure dans un fichier appel\u00e9 .gitignore (il s\u2019agit d\u2019un fichier cach\u00e9). Pour cela Aller dans le menu Repository , sous-menu Repository settings , puis Ignored Files . Copier le contenu du fichier qui se trouve \u00e0 l\u2019adresse https://github.com/github/gitignore/blob/master/Python.gitignore dans l\u2019espace r\u00e9serv\u00e9, puis Save . On doit alors Commiter les changements (le texte par d\u00e9faut qui d\u00e9crit le commit est OK). Plus de d\u00e9tails sur ces commit plus loin. \u00c9diter le fichier readme.md avec Sublime Text et enregistrer les changements localement: Basculer vers Github Desktop et discuter les changements qui interviennent sur l\u2019interface. Commiter les changements, et monter l\u2019onglet History . Pusher le travail vers gitlab en utilisant l\u2019option Publish Branch . Basculer vers gitlab et v\u00e9rifier que les changements ont bien \u00e9taient publi\u00e9s Basculer vers Spyder pour \u00e9diter du code python: Taper un programme incremente_de_un(x) et sauvegarder dans votre r\u00e9pertoire local sous le nom de fichier incremente.py . commiter les changements dans Github Desktop Refaire la m\u00eame chose avec un autre fichier ( incremente_de_deux(x) ), \u00e0 sauvegarder dans un sous r\u00e9pertoire test . Pusher son travail vers gitlab , et v\u00e9rifier que tout se met bien \u00e0 jour. De mani\u00e8re synth\u00e9tique, une fois le projet cr\u00e9\u00e9, vous modifiez vos code sources, et enregistrez r\u00e9guli\u00e8rement vos changements avec un commit (donc en local). Vous pouvez commiter des sources m\u00eame s\u2019ils ne sont pas termin\u00e9s ou comportent des bugs. \u00c0 ce stade cela n\u2019a pas d\u2019importance. quand vous \u00eates content de votre travail, et que vos jugez vos algorithmes fonctionnels, vous publiez vos pr\u00e9c\u00e9dents commit sur gitlab avec un push . Ce comportement est illustr\u00e9 par le sch\u00e9ma de la figure suivante: Pour d\u00e9truire ce projet (sans grand inter\u00eat), il faut : Quitter Spyder pour farmer les fichiers en cours. D\u00e9truire la version locale en utilisant Github Desktop : s\u00e9lectionner l\u2019onglet Current Repository , et le menu contextuel sur le nom de votre projet permet d\u2019acc\u00e9der \u00e0 Remove . Penser \u00e0 cocher la suppression physique des fichiers si vous le souhaiter (sinon seul la gestion du projet par Github Desktop sera supprim\u00e9e, les fichiers seront toujorus pr\u00e9sents sur votre disque dur). D\u00e9truire la version distante sur gitlab : Dans la barre de menu lat\u00e9rale \u00e0 gauche de l\u2019interface Web, s\u00e9lectionnez l\u2019ic\u00f4ne repr\u00e9sentant une roue crant\u00e9e correspondant \u00e0 Settings . Dans le sous-menu G\u00e9n\u00e9ral , appuyez sur le bouton Expand correspondant \u00e0 la ligne : Advanced . Tout en bas, s\u00e9lectionner Delete Project , copier la phrase demand\u00e9e et appuyer sur le bouton pour confirmer. A quoi \u00e7a sert d\u2019utiliser git et gitlab ? Les avantages sont nombreux: Avoir un copie de son travail sur un serveur distant. Sauvegarde incr\u00e9mentale et dun historique des modification (undo infini!) Partager son travail avec tout le monde (si votre projet est publique), ou avec des personnes choisies (invitations personnelles). Travailler \u00e0 plusieurs sur le m\u00eame projet et en m\u00eame temps (mais \u00e7a, c\u2019est un autre tuto\u2026) fusion automatique (ou presque) des sources tra\u00e7age des modification (qui, qoui) St\u00e9phane Derrode","title":"G\u00e9rer son premier projet git avec gitlab"}]}